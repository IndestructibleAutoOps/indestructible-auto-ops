<!-- @GL-governed -->
<!-- @GL-layer: GL90-99 -->
<!-- @GL-semantic: governed-documentation -->
<!-- @GL-audit-trail: engine/governance/GL_SEMANTIC_ANCHOR.json -->

# Production Bug Investigation & Fix Guide
## Comprehensive Step-by-Step Guide for MachineNativeOps Repository

---

## ðŸ”’ Security Warning - Handling GitHub Tokens

### âš ï¸ CRITICAL SECURITY PRACTICES

**Never commit tokens to version control:**
```bash
# Add to .gitignore (already present in your repo)
echo "GL_TOKEN" >> .gitignore
echo ".env" >> .gitignore
echo "*.secret" >> .gitignore
```

**Best Practices for Token Management:**
1. Store tokens in environment variables only
2. Use GitHub Secrets for CI/CD workflows
3. Rotate tokens regularly (every 90 days recommended)
4. Grant minimum required permissions
5. Never log tokens or expose in error messages
6. Revoke compromised tokens immediately

**Your Current Token:**
- Name: GL_TOKEN
- Type: Personal Access Token
- âš ï¸ **Action Required:** After reading this guide, rotate this token since it's now documented (see setup notes)

---

## Phase 1: Investigation Phase

### 1.1 Access and Analyze Error Logs Effectively

#### Step 1: Set Up Your Local Environment

```bash
# Navigate to repository
cd /workspace/machine-native-ops

# Install dependencies
pip install -r requirements.txt
npm install

# Set up environment variables
export GL_TOKEN=your_token_here
export LOG_LEVEL=debug
export ENVIRONMENT=development

# Verify installation
python --version  # Should be 3.11+
node --version    # Should be 20.x
```

#### Step 2: Access GitHub Issues and CI/CD Logs

```bash
# Clone and check recent issues
gh issue list --repo MachineNativeOps/machine-native-ops --limit 20 --state all

# View workflow runs
gh run list --repo MachineNativeOps/machine-native-ops --limit 20

# Get details of a failed run (replace RUN_ID)
gh run view RUN_ID --repo MachineNativeOps/machine-native-ops --log-failed

# Download logs for detailed analysis
gh run view RUN_ID --repo MachineNativeOps/machine-native-ops --log > logs/run_RUN_ID.log
```

#### Step 3: Analyze Existing Error Handling

```bash
# Find error-related files
find . -type f -name "*error*" -o -name "*exception*" | grep -v node_modules

# Review CI error analyzer
cat scripts/ci-error-analyzer.py

# Check workspace error handlers
ls -la workspace/src/core/ci_error_handler/
```

#### Step 4: Centralized Log Collection Script

Create `scripts/collect_logs.sh`:

```bash
#!/bin/bash
# Production Bug Log Collection Script

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOG_DIR="logs/bug_analysis_$TIMESTAMP"
mkdir -p "$LOG_DIR"

echo "ðŸ“Š Collecting logs for analysis..."

# 1. GitHub CI/CD Logs
echo "Collecting GitHub workflow logs..."
gh run list --limit 10 > "$LOG_DIR/recent_runs.txt"
for run_id in $(gh run list --limit 10 --json databaseId | jq -r '.[].databaseId'); do
    echo "Downloading logs for run $run_id..."
    gh run view "$run_id" --log > "$LOG_DIR/run_$run_id.log"
done

# 2. Application Logs
echo "Collecting application logs..."
find . -name "*.log" -type f -exec cp {} "$LOG_DIR/" \; 2>/dev/null

# 3. Error Reports
echo "Collecting error reports..."
find . -name "*error*" -name "*.txt" -o -name "*error*" -name "*.md" -exec cp {} "$LOG_DIR/" \; 2>/dev/null

# 4. Recent Git History
echo "Collecting recent git history..."
git log --oneline -20 > "$LOG_DIR/recent_commits.txt"

# 5. System Status
echo "Collecting system status..."
docker ps > "$LOG_DIR/docker_status.txt" 2>/dev/null
docker-compose ps > "$LOG_DIR/docker_compose_status.txt" 2>/dev/null

echo "âœ… Logs collected in: $LOG_DIR"
echo "ðŸ“‹ Log analysis file list:"
ls -lh "$LOG_DIR"

# Generate summary
cat > "$LOG_DIR/analysis_summary.txt" << EOF
=== Bug Analysis Summary ===
Collected: $(date)
Repository: MachineNativeOps/machine-native-ops
Logs Directory: $LOG_DIR

Files Collected:
$(ls -1 "$LOG_DIR" | wc -l) total files

Recommendation: Review failed runs first, then error patterns
EOF

cat "$LOG_DIR/analysis_summary.txt"
```

Make it executable:
```bash
chmod +x scripts/collect_logs.sh
./scripts/collect_logs.sh
```

---

### 1.2 Methods to Identify Patterns in Intermittent Failures

#### Pattern Detection Script

Create `scripts/detect_patterns.py`:

```python
#!/usr/bin/env python3
"""
Pattern Detection for Intermittent Failures
Analyzes logs to identify recurring error patterns
"""

import re
import os
import json
from collections import defaultdict, Counter
from datetime import datetime
from pathlib import Path

class ErrorPatternAnalyzer:
    def __init__(self, log_dir="logs"):
        self.log_dir = Path(log_dir)
        self.patterns = defaultdict(lambda: {"count": 0, "timestamps": [], "contexts": []})
        self.error_types = Counter()
        self.time_distribution = defaultdict(int)
        
    def analyze_logs(self):
        """Analyze all logs in directory"""
        log_files = list(self.log_dir.glob("*.log")) + list(self.log_dir.glob("*.txt"))
        
        for log_file in log_files:
            print(f"Analyzing: {log_file.name}")
            self._analyze_file(log_file)
        
        self._generate_report()
    
    def _analyze_file(self, log_file):
        """Analyze individual log file"""
        try:
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                lines = content.split('\n')
                
                for line in lines:
                    self._extract_patterns(line)
                    self._extract_time_patterns(line)
                    
        except Exception as e:
            print(f"Error reading {log_file}: {e}")
    
    def _extract_patterns(self, line):
        """Extract error patterns from log line"""
        # Common error patterns
        patterns = [
            r'Error:\s*(.+?)(?:\n|$)',
            r'Exception:\s*(.+?)(?:\n|$)',
            r'Failed\s+to\s+(.+?)(?:\n|$)',
            r'Timeout\s+(.+?)(?:\n|$)',
            r'Connection\s+(.+?)(?:\n|$)',
            r'race\s+condition',
            r'deadlock',
            r'concurrent',
            r'timeout',
            r'retry',
            r'attempt\s+\d+',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, line, re.IGNORECASE)
            for match in matches:
                pattern_key = match[:100]  # Limit length
                self.patterns[pattern_key]["count"] += 1
                self.patterns[pattern_key]["timestamps"].append(datetime.now().isoformat())
                self.patterns[pattern_key]["contexts"].append(line[:200])
                self.error_types[match.split()[0].lower()] += 1
    
    def _extract_time_patterns(self, line):
        """Extract time-based patterns"""
        time_patterns = [
            r'\d{2}:\d{2}:\d{2}',
            r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}',
        ]
        
        for pattern in time_patterns:
            if re.search(pattern, line):
                # Extract hour for time distribution
                hour_match = re.search(r'(\d{2}):', line)
                if hour_match:
                    hour = int(hour_match.group(1))
                    self.time_distribution[hour] += 1
                break
    
    def _generate_report(self):
        """Generate analysis report"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"pattern_analysis_report_{timestamp}.md"
        
        with open(report_file, 'w') as f:
            f.write("# Error Pattern Analysis Report\n\n")
            f.write(f"**Generated:** {datetime.now().isoformat()}\n\n")
            
            # Top error patterns
            f.write("## Top Error Patterns\n\n")
            sorted_patterns = sorted(self.patterns.items(), 
                                    key=lambda x: x[1]["count"], 
                                    reverse=True)[:20]
            
            for pattern, data in sorted_patterns:
                f.write(f"### Pattern: `{pattern}`\n")
                f.write(f"- **Occurrences:** {data['count']}\n")
                f.write(f"- **Sample Context:**\n```\n{data['contexts'][0] if data['contexts'] else 'N/A'}\n```\n\n")
            
            # Error type distribution
            f.write("## Error Type Distribution\n\n")
            for error_type, count in self.error_types.most_common(10):
                f.write(f"- **{error_type}:** {count}\n")
            
            # Time distribution
            f.write("\n## Time Distribution of Errors\n\n")
            for hour, count in sorted(self.time_distribution.items()):
                f.write(f"- **Hour {hour:02d}:** {count} errors\n")
            
            # Recommendations
            f.write("\n## Recommendations\n\n")
            f.write("Based on the analysis:\n\n")
            
            if any('timeout' in p.lower() for p in self.patterns.keys()):
                f.write("- âš ï¸ **Timeout issues detected** - Consider increasing timeout values or implementing retry logic\n")
            
            if any('race' in p.lower() for p in self.patterns.keys()):
                f.write("- âš ï¸ **Race condition indicators** - Review concurrent operations and add proper locking\n")
            
            if any('concurrent' in p.lower() for p in self.patterns.keys()):
                f.write("- âš ï¸ **Concurrency issues** - Review thread safety and synchronization\n")
            
            if self.time_distribution:
                peak_hour = max(self.time_distribution.items(), key=lambda x: x[1])
                f.write(f"- âš ï¸ **Peak error hour: {peak_hour[0]:02d}:00** - Check for scheduled tasks or load patterns\n")
        
        print(f"\nâœ… Report generated: {report_file}")
        return report_file

if __name__ == "__main__":
    import sys
    
    log_dir = sys.argv[1] if len(sys.argv) > 1 else "logs"
    
    analyzer = ErrorPatternAnalyzer(log_dir)
    analyzer.analyze_logs()
```

Run pattern detection:
```bash
python scripts/detect_patterns.py logs/bug_analysis_*
```

---

### 1.3 Steps to Reproduce the Issue in Local Development

#### Reproduction Checklist

Create `scripts/reproduce_issue.md`:

```markdown
# Issue Reproduction Checklist

## Pre-Reproduction Setup

- [ ] Fresh environment setup
- [ ] Clean git state: `git status` should show no changes
- [ ] All dependencies installed
- [ ] Environment variables configured
- [ ] Database/services running (if applicable)

## Reproduction Steps

1. **Identify the specific failure:**
   ```bash
   # List recent failures
   gh run list --repo MachineNativeOps/machine-native-ops --limit 10
   ```

2. **Get the exact workflow and commit:**
   ```bash
   # Get details of failed run
   gh run view RUN_ID --repo MachineNativeOps/machine-native-ops
   ```

3. **Checkout the failing commit:**
   ```bash
   git checkout <commit-sha>
   ```

4. **Reproduce locally:**
   ```bash
   # Run the specific workflow step locally
   npm test
   # or
   pytest tests/
   # or
   make test
   ```

5. **Enable debug logging:**
   ```bash
   export DEBUG=true
   export LOG_LEVEL=trace
   ```

6. **Run with monitoring:**
   ```bash
   # Run with strace for system calls
   strace -f -o debug_trace.log <your-command>
   
   # Run with timing information
   time <your-command>
   
   # Run with Python profiler
   python -m cProfile -o profile.stats <your-script>
   ```

## Document Findings

- [ ] Error message exactly as it appears
- [ ] Stack trace (if available)
- [ ] Steps to trigger the issue
- [ ] Frequency (always, sometimes, rarely)
- [ ] Environment details
- [ ] Related files/components
```

#### Local Testing Script

Create `scripts/test_locally.sh`:

```bash
#!/bin/bash
# Local Testing Script for Bug Reproduction

set -e

echo "ðŸ”¬ Starting local bug reproduction..."

# 1. Setup environment
echo "Setting up environment..."
export GL_TOKEN="$GL_TOKEN"
export LOG_LEVEL="debug"
export DEBUG="true"

# 2. Clean previous runs
echo "Cleaning previous test artifacts..."
rm -rf logs/*.log
rm -rf test-results/

# 3. Start services if needed
echo "Starting services..."
docker-compose up -d 2>/dev/null || echo "No docker-compose configuration found"

# 4. Run tests with detailed output
echo "Running tests..."
mkdir -p logs/test-$(date +%Y%m%d_%H%M%S)

# Run specific test suite
pytest tests/ -v --tb=long --log-cli-level=DEBUG \
    --log-cli-format="%(asctime)s [%(levelname)8s] %(message)s" \
    --log-cli-datefmt="%Y-%m-%d %H:%M:%S" \
    2>&1 | tee logs/test-output.log

# 5. Check results
if [ ${PIPESTATUS[0]} -eq 0 ]; then
    echo "âœ… Tests passed locally"
else
    echo "âŒ Tests failed locally - Review logs/test-output.log"
    exit 1
fi

# 6. Run linting
echo "Running linting..."
npm run lint 2>&1 | tee logs/lint-output.log || echo "Linting found issues"

# 7. Generate coverage
echo "Generating coverage report..."
pytest --cov=src --cov-report=html --cov-report=term

echo "ðŸ”¬ Local testing complete"
echo "ðŸ“Š Coverage report: htmlcov/index.html"
```

Make it executable:
```bash
chmod +x scripts/test_locally.sh
./scripts/test_locally.sh
```

---

## Phase 2: Debugging & Enhancement

### 2.1 Where and How to Add Comprehensive Logging

#### Logging Strategy Implementation

Create `lib/enhanced_logging.py`:

```python
#!/usr/bin/env python3
"""
Enhanced Logging Module for Production Bug Investigation
Provides structured, context-aware logging with performance tracking
"""

import logging
import sys
import time
import traceback
from functools import wraps
from datetime import datetime
from contextlib import contextmanager
from typing import Any, Dict, Optional
import json
import os

# Log levels
TRACE = 5
logging.addLevelName(TRACE, "TRACE")

class StructuredLogger:
    """Enhanced logger with structured output and context"""
    
    def __init__(self, name: str, level: int = logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)
        
        # Console handler with colored output
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(level)
        
        # File handler for persistent logs
        log_dir = os.path.join(os.getcwd(), "logs")
        os.makedirs(log_dir, exist_ok=True)
        file_handler = logging.FileHandler(
            os.path.join(log_dir, f"{name}.log")
        )
        file_handler.setLevel(logging.DEBUG)
        
        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s | %(levelname)-8s | %(name)s | %(funcName)s:%(lineno)d | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)
        
        self.logger.addHandler(console_handler)
        self.logger.addHandler(file_handler)
        
        # Context storage
        self.context: Dict[str, Any] = {}
    
    def add_context(self, **kwargs):
        """Add contextual information to logs"""
        self.context.update(kwargs)
    
    def clear_context(self):
        """Clear all context"""
        self.context.clear()
    
    def _format_message(self, message: str, **kwargs) -> str:
        """Format message with context and extra data"""
        combined = {**self.context, **kwargs}
        if combined:
            return f"{message} | Context: {json.dumps(combined)}"
        return message
    
    def trace(self, message: str, **kwargs):
        """Log at TRACE level"""
        self.logger.log(TRACE, self._format_message(message, **kwargs))
    
    def debug(self, message: str, **kwargs):
        """Log at DEBUG level"""
        self.logger.debug(self._format_message(message, **kwargs))
    
    def info(self, message: str, **kwargs):
        """Log at INFO level"""
        self.logger.info(self._format_message(message, **kwargs))
    
    def warning(self, message: str, **kwargs):
        """Log at WARNING level"""
        self.logger.warning(self._format_message(message, **kwargs))
    
    def error(self, message: str, exception: Optional[Exception] = None, **kwargs):
        """Log at ERROR level with exception details"""
        if exception:
            kwargs['exception_type'] = type(exception).__name__
            kwargs['exception_message'] = str(exception)
            kwargs['traceback'] = traceback.format_exc()
        self.logger.error(self._format_message(message, **kwargs))
    
    def critical(self, message: str, exception: Optional[Exception] = None, **kwargs):
        """Log at CRITICAL level with full details"""
        if exception:
            kwargs['exception_type'] = type(exception).__name__
            kwargs['exception_message'] = str(exception)
            kwargs['traceback'] = traceback.format_exc()
        self.logger.critical(self._format_message(message, **kwargs))

# Performance tracking decorator
def log_performance(logger: StructuredLogger):
    """Decorator to log function execution time"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            function_name = func.__name__
            
            logger.debug(f"Starting {function_name}", 
                        args=str(args)[:100], 
                        kwargs=str(kwargs)[:100])
            
            try:
                result = func(*args, **kwargs)
                elapsed_time = time.time() - start_time
                logger.info(f"Completed {function_name}", 
                           elapsed_seconds=round(elapsed_time, 3))
                return result
            except Exception as e:
                elapsed_time = time.time() - start_time
                logger.error(f"Failed {function_name}", 
                            exception=e,
                            elapsed_seconds=round(elapsed_time, 3))
                raise
        return wrapper
    return decorator

# Context manager for operation logging
@contextmanager
def log_operation(logger: StructuredLogger, operation_name: str, **context):
    """Context manager for logging operation lifecycle"""
    logger.info(f"Starting operation: {operation_name}", **context)
    start_time = time.time()
    
    try:
        yield
        elapsed_time = time.time() - start_time
        logger.info(f"Operation completed: {operation_name}", 
                   elapsed_seconds=round(elapsed_time, 3))
    except Exception as e:
        elapsed_time = time.time() - start_time
        logger.error(f"Operation failed: {operation_name}", 
                    exception=e,
                    elapsed_seconds=round(elapsed_time, 3))
        raise

# Retry mechanism with logging
def retry_with_backoff(
    logger: StructuredLogger,
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    backoff_factor: float = 2.0
):
    """Decorator for retrying failed operations with exponential backoff"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        logger.error(f"All {max_retries} attempts failed for {func.__name__}",
                                    exception=e)
                        raise
                    
                    delay = min(base_delay * (backoff_factor ** attempt), max_delay)
                    logger.warning(f"Attempt {attempt + 1}/{max_retries} failed for {func.__name__}. "
                                 f"Retrying in {delay:.1f}s...",
                                 exception=e,
                                 attempt=attempt + 1,
                                 next_retry_delay=delay)
                    time.sleep(delay)
        return wrapper
    return decorator

# Example usage
if __name__ == "__main__":
    logger = StructuredLogger("bug_investigation", level=logging.DEBUG)
    
    # Test logging
    logger.info("Application started", version="1.0.0", environment="development")
    
    # Test performance logging
    @log_performance(logger)
    def test_function(x):
        time.sleep(0.1)
        return x * 2
    
    result = test_function(5)
    logger.info(f"Result: {result}")
    
    # Test operation logging
    with log_operation(logger, "data_processing", items_count=100):
        time.sleep(0.2)
    
    # Test retry logic
    attempt_count = 0
    @retry_with_backoff(logger, max_retries=3, base_delay=0.5)
    def flaky_function():
        global attempt_count
        attempt_count += 1
        if attempt_count < 3:
            raise ValueError("Simulated failure")
        return "Success!"
    
    result = flaky_function()
    logger.info(f"Retry test result: {result}")
```

#### Integration with Existing Code

Add logging to critical paths:

```python
# Example: Add to engine/validator/error_reporter.ts

import { StructuredLogger } from '../../lib/enhanced_logging';

const logger = new StructuredLogger('validator', 'debug');

export class ErrorReporter {
  async validate(data: any) {
    logger.info('Starting validation', { dataType: typeof data, size: JSON.stringify(data).length });
    
    try {
      const result = await this.performValidation(data);
      logger.info('Validation completed', { isValid: result.valid, errorsCount: result.errors.length });
      return result;
    } catch (error) {
      logger.error('Validation failed', error, { 
        dataType: typeof data,
        errorType: error.constructor.name 
      });
      throw error;
    }
  }
  
  async performValidation(data: any) {
    logger.debug('Performing validation checks', { checks: this.getChecks().length });
    // ... validation logic
  }
}
```

---

### 2.2 How to Implement Proper Error Handling Mechanisms

#### Error Handling Framework

Create `lib/error_handling.py`:

```python
#!/usr/bin/env python3
"""
Comprehensive Error Handling Framework
Provides structured error types, handlers, and recovery strategies
"""

from typing import Optional, Callable, Any, Type, Tuple
from enum import Enum
from dataclasses import dataclass, field
import traceback
import time
from functools import wraps
from enhanced_logging import StructuredLogger

class ErrorSeverity(Enum):
    """Error severity levels"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class ErrorCategory(Enum):
    """Error categories for classification"""
    NETWORK = "network"
    DATABASE = "database"
    VALIDATION = "validation"
    BUSINESS_LOGIC = "business_logic"
    SYSTEM = "system"
    CONCURRENCY = "concurrency"
    TIMEOUT = "timeout"
    UNKNOWN = "unknown"

@dataclass
class ErrorContext:
    """Context information for errors"""
    error_id: str
    timestamp: str
    severity: ErrorSeverity
    category: ErrorCategory
    message: str
    details: dict = field(default_factory=dict)
    stack_trace: Optional[str] = None
    recoverable: bool = True
    retry_count: int = 0
    user_facing: bool = False

class ApplicationError(Exception):
    """Base class for application errors"""
    
    def __init__(
        self,
        message: str,
        severity: ErrorSeverity = ErrorSeverity.MEDIUM,
        category: ErrorCategory = ErrorCategory.UNKNOWN,
        recoverable: bool = True,
        details: dict = None,
        **kwargs
    ):
        self.message = message
        self.severity = severity
        self.category = category
        self.recoverable = recoverable
        self.details = details or {}
        self.details.update(kwargs)
        super().__init__(self.message)

class NetworkError(ApplicationError):
    """Network-related errors"""
    
    def __init__(self, message: str, **kwargs):
        super().__init__(
            message,
            category=ErrorCategory.NETWORK,
            recoverable=True,
            **kwargs
        )

class DatabaseError(ApplicationError):
    """Database-related errors"""
    
    def __init__(self, message: str, **kwargs):
        super().__init__(
            message,
            category=ErrorCategory.DATABASE,
            severity=ErrorSeverity.HIGH,
            recoverable=True,
            **kwargs
        )

class ValidationError(ApplicationError):
    """Validation errors"""
    
    def __init__(self, message: str, **kwargs):
        super().__init__(
            message,
            category=ErrorCategory.VALIDATION,
            severity=ErrorSeverity.LOW,
            recoverable=True,
            **kwargs
        )

class ConcurrencyError(ApplicationError):
    """Concurrency and race condition errors"""
    
    def __init__(self, message: str, **kwargs):
        super().__init__(
            message,
            category=ErrorCategory.CONCURRENCY,
            severity=ErrorSeverity.HIGH,
            recoverable=True,
            **kwargs
        )

class TimeoutError(ApplicationError):
    """Timeout errors"""
    
    def __init__(self, message: str, **kwargs):
        super().__init__(
            message,
            category=ErrorCategory.TIMEOUT,
            severity=ErrorSeverity.MEDIUM,
            recoverable=True,
            **kwargs
        )

class ErrorHandler:
    """Centralized error handling and recovery"""
    
    def __init__(self, logger: StructuredLogger):
        self.logger = logger
        self.error_stats = {}
        self.recovery_strategies = {
            ErrorCategory.NETWORK: self._recover_network,
            ErrorCategory.DATABASE: self._recover_database,
            ErrorCategory.TIMEOUT: self._recover_timeout,
            ErrorCategory.CONCURRENCY: self._recover_concurrency,
        }
    
    def handle_error(
        self,
        error: Exception,
        context: dict = None,
        recovery_callback: Optional[Callable] = None
    ) -> Tuple[bool, Any]:
        """
        Handle an error with appropriate recovery strategy
        
        Returns:
            Tuple[recovered, result_or_error]
        """
        error_context = self._create_error_context(error, context)
        
        # Log the error
        if isinstance(error, ApplicationError):
            self.logger.error(
                f"Application error: {error.message}",
                exception=error,
                **error_context.__dict__
            )
        else:
            self.logger.error(
                f"Unexpected error: {str(error)}",
                exception=error,
                **error_context.__dict__
            )
        
        # Update statistics
        self._update_stats(error_context)
        
        # Attempt recovery if recoverable
        if error_context.recoverable:
            if recovery_callback:
                try:
                    result = recovery_callback(error_context)
                    self.logger.info(f"Recovery successful for {error_context.error_id}")
                    return True, result
                except Exception as recovery_error:
                    self.logger.warning(f"Recovery failed for {error_context.error_id}",
                                       exception=recovery_error)
            
            # Try built-in recovery strategy
            recovery_func = self.recovery_strategies.get(error_context.category)
            if recovery_func:
                try:
                    result = recovery_func(error, error_context)
                    self.logger.info(f"Built-in recovery successful for {error_context.error_id}")
                    return True, result
                except Exception as recovery_error:
                    self.logger.warning(f"Built-in recovery failed for {error_context.error_id}",
                                       exception=recovery_error)
        
        return False, error_context
    
    def _create_error_context(
        self,
        error: Exception,
        context: dict = None
    ) -> ErrorContext:
        """Create error context from exception"""
        import uuid
        
        if isinstance(error, ApplicationError):
            return ErrorContext(
                error_id=str(uuid.uuid4()),
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
                severity=error.severity,
                category=error.category,
                message=error.message,
                details=error.details,
                stack_trace=traceback.format_exc(),
                recoverable=error.recoverable,
                **(context or {})
            )
        else:
            return ErrorContext(
                error_id=str(uuid.uuid4()),
                timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
                severity=ErrorSeverity.HIGH,
                category=ErrorCategory.UNKNOWN,
                message=str(error),
                details=context or {},
                stack_trace=traceback.format_exc(),
                recoverable=False
            )
    
    def _update_stats(self, error_context: ErrorContext):
        """Update error statistics"""
        key = f"{error_context.category.value}:{error_context.severity.value}"
        self.error_stats[key] = self.error_stats.get(key, 0) + 1
    
    def _recover_network(self, error: Exception, context: ErrorContext):
        """Network error recovery strategy"""
        self.logger.info("Attempting network recovery: implementing retry logic")
        # Implement retry with exponential backoff
        raise error  # Re-raise for retry decorator to handle
    
    def _recover_database(self, error: Exception, context: ErrorContext):
        """Database error recovery strategy"""
        self.logger.info("Attempting database recovery: checking connection")
        # Implement connection retry or fallback
        raise error
    
    def _recover_timeout(self, error: Exception, context: ErrorContext):
        """Timeout error recovery strategy"""
        self.logger.info("Attempting timeout recovery: increasing timeout")
        # Implement timeout adjustment
        raise error
    
    def _recover_concurrency(self, error: Exception, context: ErrorContext):
        """Concurrency error recovery strategy"""
        self.logger.info("Attempting concurrency recovery: implementing lock retry")
        # Implement lock retry with backoff
        raise error
    
    def get_error_summary(self) -> dict:
        """Get summary of errors handled"""
        return {
            "total_errors": sum(self.error_stats.values()),
            "by_category_severity": self.error_stats
        }

# Decorator for automatic error handling
def handle_errors(
    logger: StructuredLogger,
    error_handler: ErrorHandler,
    default_return: Any = None,
    raise_on_error: bool = False
):
    """Decorator for automatic error handling"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as error:
                recovered, result = error_handler.handle_error(
                    error,
                    context={"function": func.__name__}
                )
                
                if recovered:
                    return result
                
                if raise_on_error:
                    raise
                
                return default_return
        return wrapper
    return decorator

# Example usage
if __name__ == "__main__":
    logger = StructuredLogger("error_handling_test", level=logging.DEBUG)
    error_handler = ErrorHandler(logger)
    
    # Test error handling
    @handle_errors(logger, error_handler, default_return="Operation failed but handled")
    def risky_operation(should_fail: bool = True):
        if should_fail:
            raise NetworkError("Connection failed", endpoint="api.example.com")
        return "Success"
    
    result = risky_operation(should_fail=True)
    print(f"Result: {result}")
    
    print("\nError Summary:")
    print(error_handler.get_error_summary())
```

---

### 2.3 Techniques to Identify and Fix Potential Race Conditions

#### Race Condition Detection

Create `lib/race_condition_detector.py`:

```python
#!/usr/bin/env python3
"""
Race Condition Detection and Prevention
Tools for identifying and fixing race conditions in concurrent code
"""

import threading
import time
import inspect
from functools import wraps
from collections import defaultdict
from typing import Callable, Any, Dict, List
from enhanced_logging import StructuredLogger

class RaceConditionMonitor:
    """Monitor for detecting potential race conditions"""
    
    def __init__(self, logger: StructuredLogger):
        self.logger = logger
        self.lock_traces = defaultdict(list)
        self.shared_state_accesses = defaultdict(list)
        self.active = True
    
    def monitor_lock(self, lock: threading.Lock, lock_name: str):
        """Monitor lock acquisition and release"""
        original_acquire = lock.acquire
        original_release = lock.release
        
        def monitored_acquire(*args, **kwargs):
            calling_function = inspect.stack()[1].function
            calling_line = inspect.stack()[1].lineno
            thread_id = threading.get_ident()
            
            self.logger.trace(
                f"Attempting to acquire lock: {lock_name}",
                lock_name=lock_name,
                thread_id=thread_id,
                calling_function=calling_function,
                calling_line=calling_line
            )
            
            result = original_acquire(*args, **kwargs)
            
            if result:
                self.lock_traces[lock_name].append({
                    'thread_id': thread_id,
                    'acquired_at': time.time(),
                    'function': calling_function,
                    'line': calling_line
                })
                self.logger.trace(f"Lock acquired: {lock_name}")
            
            return result
        
        def monitored_release():
            thread_id = threading.get_ident()
            
            if self.lock_traces[lock_name]:
                trace = self.lock_traces[lock_name].pop()
                if trace['thread_id'] == thread_id:
                    self.logger.trace(f"Lock released: {lock_name}")
                else:
                    self.logger.warning(
                        f"Potential lock mismatch for {lock_name}",
                        lock_name=lock_name,
                        expected_thread=trace['thread_id'],
                        releasing_thread=thread_id
                    )
            
            return original_release()
        
        lock.acquire = monitored_acquire
        lock.release = monitored_release
    
    def check_deadlock_potential(self, timeout: float = 5.0) -> List[Dict]:
        """Check for potential deadlock situations"""
        potential_deadlocks = []
        
        for lock_name, traces in self.lock_traces.items():
            if len(traces) > 0:
                # Lock held for too long
                if time.time() - traces[-1]['acquired_at'] > timeout:
                    potential_deadlocks.append({
                        'lock_name': lock_name,
                        'thread_id': traces[-1]['thread_id'],
                        'held_duration': time.time() - traces[-1]['acquired_at'],
                        'function': traces[-1]['function'],
                        'line': traces[-1]['line']
                    })
        
        return potential_deadlocks
    
    def monitor_shared_state(self, obj: Any, state_name: str):
        """Monitor access to shared state"""
        original_getattribute = obj.__getattribute__
        
        def monitored_getattribute(name):
            if name != '__dict__' and not name.startswith('_'):
                thread_id = threading.get_ident()
                calling_function = inspect.stack()[1].function
                
                self.logger.trace(
                    f"Accessing shared state: {state_name}.{name}",
                    state_name=state_name,
                    attribute=name,
                    thread_id=thread_id,
                    calling_function=calling_function
                )
                
                self.shared_state_accesses[state_name].append({
                    'thread_id': thread_id,
                    'attribute': name,
                    'function': calling_function,
                    'timestamp': time.time()
                })
            
            return original_getattribute(name)
        
        obj.__getattribute__ = monitored_getattribute

# Thread-safe decorator
def thread_safe(logger: StructuredLogger, lock: threading.Lock = None):
    """Decorator to make a function thread-safe"""
    def decorator(func):
        func_lock = lock or threading.Lock()
        func_name = func.__name__
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            logger.trace(f"Acquiring lock for {func_name}")
            with func_lock:
                logger.trace(f"Lock acquired for {func_name}")
                try:
                    result = func(*args, **kwargs)
                    logger.trace(f"Released lock for {func_name}")
                    return result
                except Exception as e:
                    logger.error(f"Error in {func_name}", exception=e)
                    raise
        return wrapper
    return decorator

# Detect concurrent access patterns
def detect_race_conditions(logger: StructuredLogger):
    """Decorator to detect potential race conditions"""
    def decorator(func):
        access_log = defaultdict(list)
        func_name = func.__name__
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            thread_id = threading.get_ident()
            timestamp = time.time()
            calling_info = inspect.stack()[1]
            
            # Log access
            access_log[func_name].append({
                'thread_id': thread_id,
                'timestamp': timestamp,
                'function': calling_info.function,
                'line': calling_info.lineno
            })
            
            # Check for concurrent access
            recent_accesses = [
                a for a in access_log[func_name]
                if timestamp - a['timestamp'] < 0.1  # Within 100ms
            ]
            
            if len(recent_accesses) > 1:
                unique_threads = set(a['thread_id'] for a in recent_accesses)
                if len(unique_threads) > 1:
                    logger.warning(
                        f"Potential race condition detected in {func_name}",
                        function=func_name,
                        concurrent_threads=len(unique_threads),
                        access_count=len(recent_accesses)
                    )
            
            return func(*args, **kwargs)
        return wrapper
    return decorator

# Example usage
if __name__ == "__main__":
    logger = StructuredLogger("race_condition_test", level=logging.TRACE)
    monitor = RaceConditionMonitor(logger)
    
    # Create a shared resource with monitoring
    class SharedResource:
        def __init__(self):
            self.counter = 0
            self.lock = threading.Lock()
            monitor.monitor_lock(self.lock, "resource_lock")
            monitor.monitor_shared_state(self, "shared_counter")
    
    resource = SharedResource()
    
    @thread_safe(logger, resource.lock)
    def increment_counter():
        resource.counter += 1
        time.sleep(0.01)  # Simulate work
        return resource.counter
    
    # Test concurrent access
    def worker():
        for _ in range(10):
            increment_counter()
    
    threads = [threading.Thread(target=worker) for _ in range(5)]
    
    for t in threads:
        t.start()
    
    for t in threads:
        t.join()
    
    print(f"Final counter value: {resource.counter}")
    
    # Check for deadlocks
    deadlocks = monitor.check_deadlock_potential()
    if deadlocks:
        print(f"Potential deadlocks detected: {deadlocks}")
    else:
        print("No potential deadlocks detected")
```

---

## Phase 3: Solution Implementation

### 3.1 How to Add Retry Logic for Resilience

#### Comprehensive Retry Mechanism

Create `lib/retry_mechanism.py`:

```python
#!/usr/bin/env python3
"""
Comprehensive Retry Mechanism for Resilience
Implements multiple retry strategies with exponential backoff
"""

import time
import random
from functools import wraps
from typing import Callable, Any, Type, Tuple, Optional, List
from enum import Enum
from dataclasses import dataclass
from enhanced_logging import StructuredLogger
from error_handling import ApplicationError, ErrorCategory

class RetryStrategy(Enum):
    """Retry strategies"""
    EXPONENTIAL_BACKOFF = "exponential_backoff"
    LINEAR_BACKOFF = "linear_backoff"
    FIXED_DELAY = "fixed_delay"
    IMMEDIATE = "immediate"
    FIBONACCI = "fibonacci"

@dataclass
class RetryConfig:
    """Configuration for retry behavior"""
    max_attempts: int = 3
    base_delay: float = 1.0
    max_delay: float = 60.0
    backoff_factor: float = 2.0
    strategy: RetryStrategy = RetryStrategy.EXPONENTIAL_BACKOFF
    jitter: bool = True
    jitter_range: Tuple[float, float] = (0.8, 1.2)
    retryable_exceptions: List[Type[Exception]] = None
    non_retryable_exceptions: List[Type[Exception]] = None
    
    def __post_init__(self):
        if self.retryable_exceptions is None:
            self.retryable_exceptions = [ApplicationError]
        if self.non_retryable_exceptions is None:
            self.non_retryable_exceptions = []

class RetryHandler:
    """Handler for retry logic with various strategies"""
    
    def __init__(self, logger: StructuredLogger, config: RetryConfig = None):
        self.logger = logger
        self.config = config or RetryConfig()
        self.retry_stats = defaultdict(lambda: {
            'attempts': 0,
            'successes': 0,
            'failures': 0,
            'total_delay': 0.0
        })
    
    def should_retry(self, exception: Exception, attempt: int) -> bool:
        """Determine if operation should be retried"""
        # Check max attempts
        if attempt >= self.config.max_attempts:
            self.logger.info(
                f"Max retry attempts ({self.config.max_attempts}) reached"
            )
            return False
        
        # Check non-retryable exceptions
        for exc_type in self.config.non_retryable_exceptions:
            if isinstance(exception, exc_type):
                self.logger.info(
                    f"Exception {type(exception).__name__} is non-retryable"
                )
                return False
        
        # Check retryable exceptions
        for exc_type in self.config.retryable_exceptions:
            if isinstance(exception, exc_type):
                return True
        
        # Default: don't retry unknown exceptions
        return False
    
    def calculate_delay(self, attempt: int) -> float:
        """Calculate delay based on strategy"""
        delay = 0.0
        
        if self.config.strategy == RetryStrategy.EXPONENTIAL_BACKOFF:
            delay = self.config.base_delay * (self.config.backoff_factor ** attempt)
        
        elif self.config.strategy == RetryStrategy.LINEAR_BACKOFF:
            delay = self.config.base_delay * (attempt + 1)
        
        elif self.config.strategy == RetryStrategy.FIXED_DELAY:
            delay = self.config.base_delay
        
        elif self.config.strategy == RetryStrategy.IMMEDIATE:
            delay = 0.0
        
        elif self.config.strategy == RetryStrategy.FIBONACCI:
            delay = self.config.base_delay * self._fibonacci(attempt)
        
        # Cap at max delay
        delay = min(delay, self.config.max_delay)
        
        # Add jitter if enabled
        if self.config.jitter and delay > 0:
            jitter_factor = random.uniform(*self.config.jitter_range)
            delay *= jitter_factor
        
        return delay
    
    def _fibonacci(self, n: int) -> int:
        """Calculate nth Fibonacci number"""
        if n <= 0:
            return 0
        elif n == 1:
            return 1
        else:
            a, b = 0, 1
            for _ in range(2, n + 1):
                a, b = b, a + b
            return b
    
    def execute_with_retry(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Tuple[bool, Any]:
        """
        Execute function with retry logic
        
        Returns:
            Tuple[success, result_or_exception]
        """
        function_name = func.__name__
        last_exception = None
        
        for attempt in range(self.config.max_attempts):
            try:
                self.logger.debug(
                    f"Executing {function_name}",
                    attempt=attempt + 1,
                    max_attempts=self.config.max_attempts
                )
                
                result = func(*args, **kwargs)
                
                # Success
                self.retry_stats[function_name]['successes'] += 1
                self.logger.info(
                    f"{function_name} succeeded on attempt {attempt + 1}"
                )
                return True, result
                
            except Exception as e:
                last_exception = e
                self.retry_stats[function_name]['attempts'] += 1
                self.retry_stats[function_name]['failures'] += 1
                
                # Check if should retry
                if not self.should_retry(e, attempt):
                    self.logger.error(
                        f"{function_name} failed and will not be retried",
                        exception=e,
                        attempt=attempt + 1
                    )
                    return False, e
                
                # Calculate delay for next attempt
                delay = self.calculate_delay(attempt)
                self.retry_stats[function_name]['total_delay'] += delay
                
                # Log retry attempt
                self.logger.warning(
                    f"{function_name} failed on attempt {attempt + 1}. "
                    f"Retrying in {delay:.2f}s...",
                    exception=e,
                    attempt=attempt + 1,
                    next_attempt=attempt + 2,
                    delay=delay
                )
                
                # Wait before retrying
                if delay > 0:
                    time.sleep(delay)
        
        # All attempts failed
        self.logger.error(
            f"{function_name} failed after {self.config.max_attempts} attempts",
            exception=last_exception
        )
        return False, last_exception
    
    def get_stats(self) -> dict:
        """Get retry statistics"""
        return dict(self.retry_stats)

# Decorator for automatic retry
def retry(
    logger: StructuredLogger,
    config: RetryConfig = None,
    retry_handler: RetryHandler = None
):
    """Decorator for automatic retry with exponential backoff"""
    def decorator(func):
        handler = retry_handler or RetryHandler(logger, config)
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            success, result = handler.execute_with_retry(func, *args, **kwargs)
            
            if not success:
                # Re-raise the last exception
                if isinstance(result, Exception):
                    raise result
                else:
                    raise Exception(f"Function {func.__name__} failed")
            
            return result
        
        return wrapper
    return decorator

# Circuit breaker pattern
class CircuitBreaker:
    """Circuit breaker to prevent cascading failures"""
    
    def __init__(
        self,
        logger: StructuredLogger,
        failure_threshold: int = 5,
        timeout: float = 60.0,
        reset_timeout: float = 30.0
    ):
        self.logger = logger
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.reset_timeout = reset_timeout
        
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "closed"  # closed, open, half-open
    
    def call(self, func: Callable, *args, **kwargs) -> Any:
        """Execute function through circuit breaker"""
        if self.state == "open":
            if self._should_attempt_reset():
                self.state = "half-open"
                self.logger.info("Circuit breaker entering half-open state")
            else:
                raise ApplicationError(
                    "Circuit breaker is OPEN - service unavailable",
                    category=ErrorCategory.NETWORK,
                    recoverable=True
                )
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        
        except Exception as e:
            self._on_failure()
            raise
    
    def _should_attempt_reset(self) -> bool:
        """Check if should attempt to reset circuit"""
        if self.last_failure_time is None:
            return True
        
        elapsed = time.time() - self.last_failure_time
        return elapsed >= self.reset_timeout
    
    def _on_success(self):
        """Handle successful call"""
        self.failure_count = 0
        if self.state == "half-open":
            self.state = "closed"
            self.logger.info("Circuit breaker reset to closed state")
    
    def _on_failure(self):
        """Handle failed call"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = "open"
            self.logger.error(
                f"Circuit breaker opened after {self.failure_count} failures"
            )
    
    def get_state(self) -> dict:
        """Get circuit breaker state"""
        return {
            'state': self.state,
            'failure_count': self.failure_count,
            'last_failure_time': self.last_failure_time
        }

# Example usage
if __name__ == "__main__":
    logger = StructuredLogger("retry_test", level=logging.DEBUG)
    
    # Test retry with exponential backoff
    config = RetryConfig(
        max_attempts=5,
        base_delay=0.5,
        backoff_factor=2.0,
        strategy=RetryStrategy.EXPONENTIAL_BACKOFF
    )
    
    attempt_count = 0
    
    @retry(logger, config)
    def flaky_function():
        global attempt_count
        attempt_count += 1
        if attempt_count < 3:
            raise ApplicationError("Simulated failure", category=ErrorCategory.NETWORK)
        return f"Success on attempt {attempt_count}"
    
    result = flaky_function()
    print(f"Result: {result}")
    
    # Test circuit breaker
    circuit_breaker = CircuitBreaker(logger, failure_threshold=3)
    
    failure_count = 0
    @circuit_breaker.call
    def failing_service():
        global failure_count
        failure_count += 1
        raise ApplicationError("Service failure", category=ErrorCategory.NETWORK)
    
    try:
        for _ in range(5):
            failing_service()
    except Exception as e:
        print(f"Circuit breaker caught error: {e}")
    
    print(f"Circuit breaker state: {circuit_breaker.get_state()}")
```

---

### 3.2 Best Practices for Implementing the Fix

#### Implementation Best Practices Guide

Create `IMPLEMENTATION_BEST_PRACTICES.md`:

```markdown
# Implementation Best Practices

## 1. Code Review Checklist

Before implementing fixes:

- [ ] Root cause identified and documented
- [ ] Fix addresses the root cause, not just symptoms
- [ ] No new dependencies introduced without review
- [ ] Backward compatibility maintained
- [ ] Tests added for the fix
- [ ] Documentation updated
- [ ] Security implications reviewed
- [ ] Performance impact assessed

## 2. Testing Strategy

### Unit Tests
```bash
# Run specific test suite
pytest tests/unit/test_fix.py -v

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test
pytest tests/unit/test_fix.py::test_specific_scenario -v
```

### Integration Tests
```bash
# Run integration tests
pytest tests/integration/ -v

# Run against staging environment
ENVIRONMENT=staging pytest tests/integration/
```

### Load Testing
```bash
# Install locust
pip install locust

# Run load test
locust -f tests/load/test_fix.py --host=[EXTERNAL_URL_REMOVED]
```

## 3. Deployment Process

### Pre-Deployment Checklist
- [ ] All tests passing locally
- [ ] Code reviewed and approved
- [ ] Documentation updated
- [ ] Changelog updated
- [ ] Version number incremented
- [ ] Release notes prepared
- [ ] Rollback plan documented
- [ ] Monitoring alerts configured

### Safe Deployment Steps

1. **Create feature branch:**
```bash
git checkout -b fix/production-bug-$(date +%Y%m%d)
```

2. **Implement fix with logging:**
```python
# Add comprehensive logging
logger.info("Applying fix for production bug", 
           fix_id="BUG-001", 
           timestamp=datetime.now().isoformat())
```

3. **Test thoroughly:**
```bash
./scripts/test_locally.sh
```

4. **Create pull request:**
```bash
git push origin fix/production-bug-$(date +%Y%m%d)
gh pr create --title "Fix: Production Bug - [Description]" \
            --body "Fixes issue #[ISSUE_NUMBER]" \
            --base main \
            --repo MachineNativeOps/machine-native-ops
```

5. **Wait for CI/CD validation:**
```bash
gh run watch --repo MachineNativeOps/machine-native-ops
```

6. **Merge after approval:**
```bash
gh pr merge --merge --repo MachineNativeOps/machine-native-ops
```

7. **Deploy to staging first:**
```bash
gh workflow run deploy-staging.yml \
  --repo MachineNativeOps/machine-native-ops
```

8. **Monitor staging deployment:**
```bash
# Check logs
./scripts/collect_logs.sh

# Monitor metrics
# (See monitoring section below)
```

9. **Deploy to production:**
```bash
gh workflow run deploy-production.yml \
  --repo MachineNativeOps/machine-native-ops
```

10. **Verify production deployment:**
```bash
# Run smoke tests
./scripts/smoke_test.sh

# Monitor for errors
./scripts/monitor_production.sh
```

## 4. Rollback Procedure

If issues arise after deployment:

```bash
# 1. Identify the problematic commit
git log --oneline -10

# 2. Rollback to previous stable version
git revert HEAD

# 3. Push rollback
git push origin main

# 4. Monitor rollback
gh run watch --repo MachineNativeOps/machine-native-ops
```

## 5. Documentation Requirements

Update the following documentation:

### CHANGELOG.md
```markdown
## [VERSION] - YYYY-MM-DD

### Fixed
- Fixed production bug causing intermittent failures
  - Added retry logic for network operations
  - Improved error handling and logging
  - Added race condition detection
```

### README.md
Add any new environment variables or configuration options.

### API Documentation
Update API docs if any endpoints changed.

### Operations Documentation
Update runbooks and troubleshooting guides.

## 6. Code Quality Standards

### Python Code Style
```bash
# Run linters
flake8 src/
black src/
isort src/

# Type checking
mypy src/
```

### TypeScript Code Style
```bash
# Run linters
npm run lint
npm run format
npm run typecheck
```

## 7. Security Considerations

- [ ] No secrets in code
- [ ] Input validation on all user inputs
- [ ] Output encoding to prevent XSS
- [ ] SQL injection prevention
- [ ] Authentication and authorization checks
- [ ] Rate limiting implemented
- [ ] Audit logging enabled

## 8. Performance Optimization

- [ ] No N+1 queries
- [ ] Proper indexing in database
- [ ] Caching strategy implemented
- [ ] Async operations where appropriate
- [ ] Resource limits set
- [ ] Memory leaks checked

## 9. Monitoring and Alerting

See Phase 4 for comprehensive monitoring setup.

## 10. Post-Implementation Review

After fix is deployed:

- [ ] Monitor for 24-48 hours
- [ ] Collect metrics
- [ ] Review logs for any issues
- [ ] Gather feedback from users
- [ ] Document lessons learned
- [ ] Update runbooks
- [ ] Schedule follow-up review
```

---

## Phase 4: Deployment & Monitoring

### 4.1 Steps to Deploy a Hotfix Safely to Production

#### Safe Hotfix Deployment Script

Create `scripts/deploy_hotfix.sh`:

```bash
#!/bin/bash
# Safe Hotfix Deployment Script

set -e

# Configuration
REPO="MachineNativeOps/machine-native-ops"
HOTFIX_BRANCH="hotfix/production-$(date +%Y%m%d-%H%M%S)"
LOG_FILE="logs/hotfix-deployment-$(date +%Y%m%d_%H%M%S).log"

mkdir -p logs

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log() {
    echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} $1" | tee -a "$LOG_FILE"
}

error() {
    echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')] ERROR:${NC} $1" | tee -a "$LOG_FILE"
}

warning() {
    echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')] WARNING:${NC} $1" | tee -a "$LOG_FILE"
}

# Pre-deployment checks
pre_deployment_checks() {
    log "=== Pre-deployment Checks ==="
    
    # Check if we're on main branch
    current_branch=$(git branch --show-current)
    if [ "$current_branch" != "main" ]; then
        error "Not on main branch. Current branch: $current_branch"
        exit 1
    fi
    
    # Check for uncommitted changes
    if [ -n "$(git status --porcelain)" ]; then
        error "Uncommitted changes detected"
        git status --short
        exit 1
    fi
    
    # Check if GL_TOKEN is set
    if [ -z "$GL_TOKEN" ]; then
        error "GL_TOKEN environment variable not set"
        exit 1
    fi
    
    # Run tests
    log "Running tests..."
    if ! ./scripts/test_locally.sh >> "$LOG_FILE" 2>&1; then
        error "Tests failed"
        exit 1
    fi
    
    log "âœ… All pre-deployment checks passed"
}

# Create hotfix branch
create_hotfix_branch() {
    log "=== Creating Hotfix Branch ==="
    
    git checkout -b "$HOTFIX_BRANCH"
    log "Created branch: $HOTFIX_BRANCH"
}

# Implement fix (user will edit files)
implement_fix() {
    log "=== Implementing Fix ==="
    warning "Please implement your fix now."
    warning "Press Enter when ready to continue..."
    read -r
    
    # Run tests again
    log "Running tests with fix..."
    if ! ./scripts/test_locally.sh >> "$LOG_FILE" 2>&1; then
        error "Tests failed after fix implementation"
        exit 1
    fi
    
    log "âœ… Fix implemented and tested"
}

# Commit changes
commit_changes() {
    log "=== Committing Changes ==="
    
    git add -A
    git commit -m "Hotfix: Production bug fix

- Fixed intermittent failures in production
- Added comprehensive logging
- Implemented retry logic
- Enhanced error handling

Deployment: $(date)"
    
    log "âœ… Changes committed"
}

# Push to remote
push_to_remote() {
    log "=== Pushing to Remote ==="
    
    git push -u origin "$HOTFIX_BRANCH"
    log "âœ… Pushed branch: $HOTFIX_BRANCH"
}

# Create pull request
create_pull_request() {
    log "=== Creating Pull Request ==="
    
    gh pr create \
        --title "HOTFIX: Production Bug Fix - $(date +%Y-%m-%d)" \
        --body "## Hotfix Details

### Problem
Production environment experiencing intermittent failures.

### Solution
- Enhanced error handling and logging
- Implemented retry logic with exponential backoff
- Added race condition detection
- Improved monitoring and alerting

### Testing
- All tests passing locally
- Staging deployment verified

### Risk Assessment
- **Severity**: HIGH
- **Impact**: Production stability
- **Rollback Plan**: Revert commit if issues arise

### Checklist
- [x] Root cause identified
- [x] Fix implemented
- [x] Tests passing
- [x] Documentation updated
- [x] Rollback plan documented

**Deployment**: Pending approval
**Requester**: $USER
**Date**: $(date)" \
        --base main \
        --repo "$REPO"
    
    log "âœ… Pull request created"
}

# Wait for CI/CD
wait_for_ci_cd() {
    log "=== Waiting for CI/CD ==="
    
    warning "Waiting for CI/CD checks to pass..."
    gh run watch --repo "$REPO" --exit-status
    
    log "âœ… CI/CD checks passed"
}

# Merge pull request
merge_pull_request() {
    log "=== Merging Pull Request ==="
    
    pr_number=$(gh pr list --repo "$REPO" --head "$HOTFIX_BRANCH" --json number --jq '.[0].number')
    
    gh pr merge "$pr_number" --merge --repo "$REPO"
    
    log "âœ… Pull request merged"
}

# Deploy to staging
deploy_to_staging() {
    log "=== Deploying to Staging ==="
    
    gh workflow run deploy-staging.yml --repo "$REPO"
    
    log "Waiting for staging deployment..."
    sleep 30
    
    # Monitor staging deployment
    log "Monitoring staging deployment..."
    gh run watch --repo "$REPO" --exit-status
    
    log "âœ… Deployed to staging"
}

# Verify staging deployment
verify_staging() {
    log "=== Verifying Staging Deployment ==="
    
    warning "Please verify the staging deployment manually."
    warning "Press Enter when verified successfully..."
    read -r
    
    log "âœ… Staging deployment verified"
}

# Deploy to production
deploy_to_production() {
    log "=== Deploying to Production ==="
    
    warning "âš ï¸  ABOUT TO DEPLOY TO PRODUCTION âš ï¸"
    warning "This is a critical operation. Are you sure?"
    warning "Type 'yes' to confirm:"
    read -r confirmation
    
    if [ "$confirmation" != "yes" ]; then
        error "Deployment cancelled by user"
        exit 1
    fi
    
    gh workflow run deploy-production.yml --repo "$REPO"
    
    log "Waiting for production deployment..."
    sleep 30
    
    # Monitor production deployment
    log "Monitoring production deployment..."
    gh run watch --repo "$REPO" --exit-status
    
    log "âœ… Deployed to production"
}

# Verify production deployment
verify_production() {
    log "=== Verifying Production Deployment ==="
    
    log "Running production smoke tests..."
    if ./scripts/smoke_test.sh >> "$LOG_FILE" 2>&1; then
        log "âœ… Smoke tests passed"
    else
        error "Smoke tests failed"
        warning "Consider rolling back"
        exit 1
    fi
    
    log "âœ… Production deployment verified"
}

# Post-deployment monitoring
post_deployment_monitoring() {
    log "=== Post-deployment Monitoring ==="
    
    log "Monitoring production for 5 minutes..."
    ./scripts/monitor_production.sh 300
    
    log "âœ… Post-deployment monitoring complete"
}

# Cleanup
cleanup() {
    log "=== Cleanup ==="
    
    # Switch back to main
    git checkout main
    
    # Delete local branch
    git branch -d "$HOTFIX_BRANCH" 2>/dev/null || warning "Could not delete local branch"
    
    log "âœ… Cleanup complete"
}

# Main execution
main() {
    log "=== Starting Hotfix Deployment ==="
    log "Repository: $REPO"
    log "Branch: $HOTFIX_BRANCH"
    
    pre_deployment_checks
    create_hotfix_branch
    implement_fix
    commit_changes
    push_to_remote
    create_pull_request
    wait_for_ci_cd
    merge_pull_request
    deploy_to_staging
    verify_staging
    deploy_to_production
    verify_production
    post_deployment_monitoring
    cleanup
    
    log "=== Hotfix Deployment Complete ==="
    log "Log file: $LOG_FILE"
}

# Run main function
main
```

Make it executable:
```bash
chmod +x scripts/deploy_hotfix.sh
```

---

### 4.2 How to Set Up Monitoring to Verify the Fix and Prevent Recurrence

#### Comprehensive Monitoring Setup

Create `scripts/monitor_production.sh`:

```bash
#!/bin/bash
# Production Monitoring Script

MONITOR_DURATION=${1:-300}  # Default 5 minutes
INTERVAL=30  # Check every 30 seconds
LOG_FILE="logs/production-monitoring-$(date +%Y%m%d_%H%M%S).log"

mkdir -p logs

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log() {
    echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} $1" | tee -a "$LOG_FILE"
}

error() {
    echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')] ERROR:${NC} $1" | tee -a "$LOG_FILE"
}

warning() {
    echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')] WARNING:${NC} $1" | tee -a "$LOG_FILE"
}

info() {
    echo -e "${BLUE}[$(date '+%Y-%m-%d %H:%M:%S')] INFO:${NC} $1" | tee -a "$LOG_FILE"
}

# Check GitHub Actions workflow runs
check_workflows() {
    info "Checking GitHub Actions workflow runs..."
    
    # Get recent runs
    recent_runs=$(gh run list --repo MachineNativeOps/machine-native-ops --limit 10 --json databaseId,status,conclusion,name)
    
    # Check for failures
    failed_runs=$(echo "$recent_runs" | jq -r '.[] | select(.status == "completed" and .conclusion == "failure") | .databaseId')
    
    if [ -n "$failed_runs" ]; then
        error "Failed workflow runs detected:"
        echo "$failed_runs" | while read -r run_id; do
            error "  - Run $run_id"
            gh run view "$run_id" --repo MachineNativeOps/machine-native-ops --log-failed >> "$LOG_FILE" 2>&1
        done
        return 1
    else
        log "âœ… No failed workflow runs detected"
        return 0
    fi
}

# Check application logs
check_application_logs() {
    info "Checking application logs..."
    
    # Check for errors in recent logs
    error_count=$(find logs/ -name "*.log" -newermt "5 minutes ago" -exec grep -l "ERROR\|CRITICAL" {} \; 2>/dev/null | wc -l)
    
    if [ "$error_count" -gt 0 ]; then
        warning "Found $error_count log files with recent errors"
        find logs/ -name "*.log" -newermt "5 minutes ago" -exec grep -h "ERROR\|CRITICAL" {} \; 2>/dev/null | tail -10 | tee -a "$LOG_FILE"
        return 1
    else
        log "âœ… No recent errors in application logs"
        return 0
    fi
}

# Check system resources
check_system_resources() {
    info "Checking system resources..."
    
    # Check CPU usage
    cpu_usage=$(top -bn1 | grep "Cpu(s)" | sed "s/.*, *\([0-9.]*\)%* id.*/\1/" | awk '{print 100 - $1}')
    if (( $(echo "$cpu_usage > 80" | bc -l) )); then
        warning "High CPU usage: ${cpu_usage}%"
    else
        log "âœ… CPU usage normal: ${cpu_usage}%"
    fi
    
    # Check memory usage
    memory_usage=$(free | grep Mem | awk '{print ($3/$2) * 100.0}')
    if (( $(echo "$memory_usage > 80" | bc -l) )); then
        warning "High memory usage: ${memory_usage}%"
    else
        log "âœ… Memory usage normal: ${memory_usage}%"
    fi
    
    # Check disk usage
    disk_usage=$(df -h / | awk 'NR==2 {print $5}' | sed 's/%//')
    if [ "$disk_usage" -gt 80 ]; then
        warning "High disk usage: ${disk_usage}%"
    else
        log "âœ… Disk usage normal: ${disk_usage}%"
    fi
}

# Check for specific error patterns
check_error_patterns() {
    info "Checking for specific error patterns..."
    
    patterns=(
        "race condition"
        "deadlock"
        "timeout"
        "connection refused"
        "segmentation fault"
        "out of memory"
    )
    
    found_errors=false
    for pattern in "${patterns[@]}"; do
        count=$(find logs/ -name "*.log" -newermt "5 minutes ago" -exec grep -i "$pattern" {} \; 2>/dev/null | wc -l)
        if [ "$count" -gt 0 ]; then
            error "Found $count occurrences of pattern: $pattern"
            find logs/ -name "*.log" -newermt "5 minutes ago" -exec grep -i "$pattern" {} \; 2>/dev/null | tail -5 | tee -a "$LOG_FILE"
            found_errors=true
        fi
    done
    
    if [ "$found_errors" = false ]; then
        log "âœ… No concerning error patterns detected"
    fi
}

# Generate monitoring report
generate_report() {
    info "Generating monitoring report..."
    
    report_file="logs/monitoring-report-$(date +%Y%m%d_%H%M%S).html"
    
    cat > "$report_file" << EOF
<!DOCTYPE html>
<html>
<head>
    <title>Production Monitoring Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .header { background: #f0f0f0; padding: 20px; margin-bottom: 20px; }
        .section { margin-bottom: 30px; }
        .success { color: green; }
        .error { color: red; }
        .warning { color: orange; }
        .info { color: blue; }
        .log { background: #f9f9f9; padding: 10px; overflow-x: auto; }
    </style>
</head>
<body>
    <div class="header">
        <h1>Production Monitoring Report</h1>
        <p><strong>Generated:</strong> $(date)</p>
        <p><strong>Monitoring Duration:</strong> $MONITOR_DURATION seconds</p>
    </div>
    
    <div class="section">
        <h2>Summary</h2>
        <ul>
            <li class="success">âœ… Workflow status: $(check_workflows 2>&1 | tail -1)</li>
            <li class="success">âœ… Application logs: $(check_application_logs 2>&1 | tail -1)</li>
            <li class="info">â„¹ï¸  System resources checked</li>
            <li class="info">â„¹ï¸  Error patterns analyzed</li>
        </ul>
    </div>
    
    <div class="section">
        <h2>Detailed Logs</h2>
        <div class="log">
            <pre>$(cat "$LOG_FILE")</pre>
        </div>
    </div>
</body>
</html>
EOF
    
    log "âœ… Report generated: $report_file"
}

# Main monitoring loop
main() {
    log "=== Starting Production Monitoring ==="
    log "Duration: $MONITOR_DURATION seconds"
    log "Interval: $INTERVAL seconds"
    
    end_time=$(($(date +%s) + MONITOR_DURATION))
    
    while [ $(date +%s) -lt $end_time ]; do
        log "--- Monitoring Check ---"
        
        check_workflows
        check_application_logs
        check_system_resources
        check_error_patterns
        
        if [ $(date +%s) -lt $end_time ]; then
            sleep $INTERVAL
        fi
    done
    
    generate_report
    
    log "=== Monitoring Complete ==="
    log "Log file: $LOG_FILE"
}

# Run main function
main
```

Create monitoring dashboard configuration:

Create `config/monitoring/prometheus.yml`:

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'machine-native-ops'
    static_configs:
      - targets: ['localhost:9090']
    metrics_path: '/metrics'
    scrape_interval: 10s

  - job_name: 'github-actions'
    static_configs:
      - targets: ['localhost:8080']
    metrics_path: '/github-metrics'
```

Create alert rules:

Create `config/monitoring/alert_rules.yml`:

```yaml
groups:
  - name: production_alerts
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: rate(error_count[5m]) > 10
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors/second for the last 5 minutes"
      
      - alert: WorkflowFailures
        expr: github_actions_failed_total > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "GitHub Actions workflow failures"
          description: "{{ $value }} failed workflows in the last minute"
      
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value }}%"
      
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value }}%"
      
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 15
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space"
          description: "Disk space is {{ $value }}% available"
      
      - alert: RaceConditionDetected
        expr: rate(race_condition_errors[1m]) > 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "Race condition detected"
          description: "Race condition errors detected at rate {{ $value }}"
      
      - alert: TimeoutErrors
        expr: rate(timeout_errors[5m]) > 5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High timeout error rate"
          description: "Timeout errors at rate {{ $value }}/second"
```

#### Grafana Dashboard Configuration

Create `config/monitoring/grafana_dashboard.json`:

```json
{
  "dashboard": {
    "title": "MachineNativeOps Production Monitoring",
    "tags": ["production", "monitoring"],
    "timezone": "browser",
    "panels": [
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(error_count[5m])",
            "legendFormat": "Errors/sec"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {"params": [10], "type": "gt"},
              "operator": {"type": "and"},
              "query": {"params": ["A", "5m", "now"]},
              "reducer": {"params": [], "type": "avg"},
              "type": "query"
            }
          ]
        }
      },
      {
        "title": "GitHub Actions Status",
        "type": "stat",
        "targets": [
          {
            "expr": "github_actions_failed_total",
            "legendFormat": "Failed"
          },
          {
            "expr": "github_actions_success_total",
            "legendFormat": "Success"
          }
        ]
      },
      {
        "title": "System Resources",
        "type": "graph",
        "targets": [
          {
            "expr": "100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=&quot;idle&quot;}[5m])) * 100)",
            "legendFormat": "CPU %"
          },
          {
            "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100",
            "legendFormat": "Memory %"
          }
        ]
      },
      {
        "title": "Race Condition Errors",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(race_condition_errors[1m])",
            "legendFormat": "Race conditions/sec"
          }
        ]
      },
      {
        "title": "Timeout Errors",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(timeout_errors[5m])",
            "legendFormat": "Timeouts/sec"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          }
        ]
      }
    ]
  }
}
```

#### Setup Monitoring Stack

Create `scripts/setup_monitoring.sh`:

```bash
#!/bin/bash
# Setup Monitoring Stack (Prometheus + Grafana)

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
}

log "=== Setting Up Monitoring Stack ==="

# Check Docker
if ! command -v docker &> /dev/null; then
    log "Error: Docker not installed"
    exit 1
fi

# Create docker-compose for monitoring
cat > docker-compose.monitoring.yml << EOF
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./config/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./config/monitoring/alert_rules.yml:/etc/prometheus/alert_rules.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/monitoring/grafana_dashboard.json:/etc/grafana/provisioning/dashboards/dashboard.json
    restart: unless-stopped

  alertmanager:
    image: prom/alertmanager:latest
    ports:
      - "9093:9093"
    volumes:
      - ./config/monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    restart: unless-stopped

volumes:
  prometheus_data:
  grafana_data:
EOF

# Create alertmanager config
mkdir -p config/monitoring
cat > config/monitoring/alertmanager.yml << EOF
global:
  resolve_timeout: 5m

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'

receivers:
  - name: 'web.hook'
    webhook_configs:
      - url: '[EXTERNAL_URL_REMOVED]
EOF

log "Starting monitoring stack..."
docker-compose -f docker-compose.monitoring.yml up -d

log "Waiting for services to start..."
sleep 10

log "=== Monitoring Stack Setup Complete ==="
log "Prometheus: [EXTERNAL_URL_REMOVED]
log "Grafana: [EXTERNAL_URL_REMOVED] (admin/admin)"
log "Alertmanager: [EXTERNAL_URL_REMOVED]

# Expose ports for external access
log "Exposing monitoring ports..."
(expose-port 9090 > /dev/null 2>&1 &)
(expose-port 3000 > /dev/null 2>&1 &)
(expose-port 9093 > /dev/null 2>&1 &)

log "âœ… Monitoring stack ready"
```

---

## Final Checklist

Before considering the bug fix complete:

### âœ… Pre-Deployment
- [ ] Root cause identified and documented
- [ ] Fix implemented with comprehensive logging
- [ ] All tests passing (unit, integration, load)
- [ ] Code reviewed and approved
- [ ] Documentation updated
- [ ] Rollback plan documented
- [ ] Staging deployment verified
- [ ] Performance impact assessed

### âœ… Deployment
- [ ] Hotfix deployed to production
- [ ] Smoke tests passing
- [ ] No immediate errors detected
- [ ] System resources stable

### âœ… Post-Deployment
- [ ] Monitoring active for 24-48 hours
- [ ] Error rate at acceptable levels
- [ ] No new issues introduced
- [ ] Performance metrics normal
- [ ] Alerts configured and tested
- [ ] Documentation finalized
- [ ] Lessons learned documented

### âœ… Long-term
- [ ] Root cause analysis published
- [ ] Preventive measures implemented
- [ ] Monitoring dashboards configured
- [ ] Alert thresholds tuned
- [ ] Runbooks updated
- [ ] Team briefed on fix

---

## Summary

This comprehensive guide provides:

1. **Investigation Phase:** Tools and techniques to identify the root cause
2. **Debugging & Enhancement:** Comprehensive logging and error handling
3. **Solution Implementation:** Retry logic and best practices
4. **Deployment & Monitoring:** Safe deployment procedures and monitoring setup

By following this guide, you'll be able to:
- Systematically investigate production bugs
- Implement robust fixes with proper logging
- Deploy safely to production
- Monitor effectively to prevent recurrence

Remember: **Security First** - Rotate your GitHub token immediately after reading this guide!