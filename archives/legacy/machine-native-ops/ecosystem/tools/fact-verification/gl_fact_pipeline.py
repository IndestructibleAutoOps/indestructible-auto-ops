#!/usr/bin/env python3
#
# @GL-governed
# @GL-layer: GL10-29
# @GL-semantic: tools
# @GL-audit-trail: ../../governance/GL_SEMANTIC_ANCHOR.json
#
"""
GLäº‹å®éªŒè¯ç®¡é“æ‰§è¡Œè„šæœ¬
ç‰ˆæœ¬: 1.0.0
ç”¨é€”: æ‰§è¡Œä¸‰å±‚éªŒè¯ç®¡é“ï¼Œç”ŸæˆåŸºäºå¯éªŒè¯äº‹å®çš„æŠ¥å‘Š
"""

import yaml
import hashlib
import json
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum


class Severity(Enum):
    CRITICAL = "CRITICAL"
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"


class EvidenceType(Enum):
    CONTRACT = "contract"
    REGISTRY = "registry"
    GOVERNANCE = "governance"
    ONTOLOGY = "ontology"
    ACTUAL_STATE = "actual_state"


class DifferenceCategory(Enum):
    ALIGNED = "aligned"
    INTENTIONAL_DEVIATION = "intentional-deviation"
    ALTERNATIVE_IMPLEMENTATION = "alternative-implementation"
    TECHNICAL_DEBT = "technical-debt"
    GAP = "gap"
    EXTENSION = "extension"


@dataclass
class ValidationResult:
    passed: bool
    errors: List[str]
    warnings: List[str]


@dataclass
class InternalSource:
    path: str
    hash: str
    size: int
    last_modified: str
    kind: str
    version: Optional[str] = None


@dataclass
class ExternalReference:
    standard_name: str
    standard_version: str
    source_url: str
    last_updated: str
    applicability: str
    reliability_level: str


@dataclass
class Difference:
    category: DifferenceCategory
    description: str
    internal_evidence: str
    external_reference: str
    rationale: Optional[str] = None
    action: Optional[str] = None


class GLFactPipeline:
    def __init__(self, config_path: str, workspace_path: str = "."):
        self.config_path = Path(config_path)
        self.workspace_path = Path(workspace_path)
        
        # åŠ è½½é…ç½®
        with open(self.config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.internal_facts: Dict = {}
        self.external_context: Dict = {}
        self.validation_errors: List[str] = []
        self.validation_warnings: List[str] = []
        
        # ç¦æ­¢çš„çŸ­è¯­
        self.forbidden_phrases = [
            (r"100% å®Œæˆ", "åŸºäºå·²å®ç°çš„åŠŸèƒ½é›†"),
            (r"å®Œå…¨ç¬¦åˆ", "åœ¨[æ–¹é¢]ä¸æ ‡å‡†å¯¹é½"),
            (r"å·²å…¨éƒ¨å®ç°", "å·²å®ç°[å…·ä½“åŠŸèƒ½åˆ—è¡¨]"),
            (r"è¦†ç›–æ‰€æœ‰æ ‡å‡†", "è¦†ç›–[å…·ä½“æ ‡å‡†åˆ—è¡¨]"),
            (r"åº”è¯¥æ˜¯", "æ ¹æ®[è¯æ®]ï¼Œå»ºè®®"),
            (r"å¯èƒ½æ˜¯", "åŸºäº[è¯æ®]ï¼Œæ¨æµ‹"),
            (r"æˆ‘è®¤ä¸º", "åŸºäº[è¯æ®]ï¼Œåˆ†æè¡¨æ˜"),
            (r"æ¨æµ‹", "åŸºäº[è¯æ®]ï¼Œæ¨æ–­"),
        ]
    
    def validate_internal_source(self, path: Path) -> ValidationResult:
        """éªŒè¯å†…éƒ¨æºæ–‡ä»¶"""
        errors = []
        warnings = []
        
        if not path.exists():
            errors.append(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return ValidationResult(False, errors, warnings)
        
        if not path.is_file():
            errors.append(f"ä¸æ˜¯æ–‡ä»¶: {path}")
            return ValidationResult(False, errors, warnings)
        
        try:
            with open(path, 'r') as f:
                content = f.read()
                
            # è®¡ç®— SHA-256 å“ˆå¸Œ
            file_hash = hashlib.sha256(content.encode()).hexdigest()
            
            # å¦‚æœæ˜¯ YAML æ–‡ä»¶ï¼Œå°è¯•è§£æ
            if path.suffix in ['.yaml', '.yml']:
                try:
                    yaml_data = yaml.safe_load(content)
                    if yaml_data:
                        # æ£€æŸ¥å¿…éœ€å­—æ®µ
                        if 'metadata' in yaml_data:
                            if 'version' not in yaml_data['metadata']:
                                warnings.append(f"ç¼ºå°‘ version å­—æ®µ: {path}")
                            if 'kind' not in yaml_data['metadata']:
                                warnings.append(f"ç¼ºå°‘ kind å­—æ®µ: {path}")
                except yaml.YAMLError as e:
                    errors.append(f"YAML è§£æå¤±è´¥ {path}: {e}")
            
            return ValidationResult(True, [], warnings)
            
        except Exception as e:
            errors.append(f"è¯»å–å¤±è´¥ {path}: {e}")
            return ValidationResult(False, errors, warnings)
    
    def collect_internal_facts(self) -> Dict:
        """ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†å†…éƒ¨äº‹å®"""
        print("ğŸ” é˜¶æ®µ1: æ”¶é›†å†…éƒ¨äº‹å®...")
        
        facts = {
            "timestamp": datetime.utcnow().isoformat(),
            "sources": [],
            "contracts": {},
            "registry_state": {},
            "root_hash": None
        }
        
        # æ”¶é›†å¥‘çº¦
        contracts_path = self.workspace_path / "ecosystem/contracts"
        if contracts_path.exists():
            print(f"  ğŸ“‚ æ‰«æå¥‘çº¦ç›®å½•: {contracts_path}")
            for contract_file in contracts_path.glob("**/*.yaml"):
                validation = self.validate_internal_source(contract_file)
                if validation.passed:
                    with open(contract_file, 'r') as f:
                        content = f.read()
                        file_hash = hashlib.sha256(content.encode()).hexdigest()
                        
                        source = InternalSource(
                            path=str(contract_file.relative_to(self.workspace_path)),
                            hash=file_hash,
                            size=len(content),
                            last_modified=datetime.fromtimestamp(
                                contract_file.stat().st_mtime
                            ).isoformat(),
                            kind="contract"
                        )
                        facts["sources"].append(asdict(source))
                        
                        # æå–å¥‘çº¦ä¿¡æ¯
                        try:
                            yaml_data = yaml.safe_load(content)
                            if yaml_data and 'metadata' in yaml_data:
                                contract_id = yaml_data['metadata'].get('name', contract_file.stem)
                                facts["contracts"][contract_id] = {
                                    "version": yaml_data['metadata'].get('version', 'unknown'),
                                    "kind": yaml_data['metadata'].get('kind', 'unknown'),
                                    "path": str(contract_file.relative_to(self.workspace_path)),
                                    "hash": file_hash
                                }
                        except:
                            pass
                else:
                    self.validation_errors.extend(validation.errors)
                    self.validation_warnings.extend(validation.warnings)
        
        # æ”¶é›†æ³¨å†Œè¡¨
        registry_path = self.workspace_path / "ecosystem/registry"
        if registry_path.exists():
            print(f"  ğŸ“‚ æ‰«ææ³¨å†Œè¡¨ç›®å½•: {registry_path}")
            for registry_file in registry_path.glob("**/*.yaml"):
                validation = self.validate_internal_source(registry_file)
                if validation.passed:
                    with open(registry_file, 'r') as f:
                        content = f.read()
                        file_hash = hashlib.sha256(content.encode()).hexdigest()
                        
                        source = InternalSource(
                            path=str(registry_file.relative_to(self.workspace_path)),
                            hash=file_hash,
                            size=len(content),
                            last_modified=datetime.fromtimestamp(
                                registry_file.stat().st_mtime
                            ).isoformat(),
                            kind="registry"
                        )
                        facts["sources"].append(asdict(source))
        
        # è®¡ç®—æ ¹å“ˆå¸Œ
        all_hashes = [s['hash'] for s in facts["sources"]]
        if all_hashes:
            combined = "".join(sorted(all_hashes))
            facts["root_hash"] = hashlib.sha256(combined.encode()).hexdigest()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        facts["statistics"] = {
            "total_files": len(facts["sources"]),
            "total_size": sum(s["size"] for s in facts["sources"]),
            "contracts_count": len(facts["contracts"]),
            "validation_errors": len(self.validation_errors),
            "validation_warnings": len(self.validation_warnings)
        }
        
        self.internal_facts = facts
        
        print(f"  âœ… æ”¶é›†å®Œæˆ: {len(facts['sources'])} ä¸ªæ–‡ä»¶")
        print(f"  ğŸ“Š ç»Ÿè®¡: {facts['statistics']}")
        
        return facts
    
    def collect_external_context(self, topics: List[str] = None) -> Dict:
        """ç¬¬äºŒé˜¶æ®µï¼šæ”¶é›†å¤–éƒ¨è¯­å¢ƒ"""
        print("\nğŸŒ é˜¶æ®µ2: æ”¶é›†å¤–éƒ¨è¯­å¢ƒ...")
        
        if topics is None:
            topics = ["semver", "cncf", "togaf"]
        
        context = {
            "collected_at": datetime.utcnow().isoformat(),
            "version": "1.0.0",
            "references": [],
            "standard_requirements": []
        }
        
        # æ¨¡æ‹Ÿå¤–éƒ¨æ ‡å‡†æ”¶é›†
        standards_db = {
            "semver": {
                "name": "Semantic Versioning",
                "url": "https://semver.org/",
                "version": "2.0.0",
                "reliability": "high",
                "requirements": [
                    {
                        "id": "SEMVER-001",
                        "text": "ç‰ˆæœ¬æ ¼å¼å¿…é¡»æ˜¯ MAJOR.MINOR.PATCH",
                        "mandatory": True
                    }
                ]
            },
            "cncf": {
                "name": "Cloud Native Computing Foundation",
                "url": "https://www.cncf.io/",
                "version": "latest",
                "reliability": "high",
                "requirements": [
                    {
                        "id": "CNCF-001",
                        "text": "å®¹å™¨åŒ–éƒ¨ç½²",
                        "mandatory": True
                    }
                ]
            },
            "togaf": {
                "name": "The Open Group Architecture Framework",
                "url": "https://www.opengroup.org/togaf/",
                "version": "9.2",
                "reliability": "high",
                "requirements": [
                    {
                        "id": "TOGAF-001",
                        "text": "æ¶æ„æ²»ç†",
                        "mandatory": True
                    }
                ]
            }
        }
        
        for topic in topics:
            if topic in standards_db:
                std = standards_db[topic]
                ref = ExternalReference(
                    standard_name=std["name"],
                    standard_version=std["version"],
                    source_url=std["url"],
                    last_updated="2024-01-01",
                    applicability="reference",
                    reliability_level=std["reliability"]
                )
                context["references"].append(asdict(ref))
                context["standard_requirements"].extend(std["requirements"])
        
        self.external_context = context
        
        print(f"  âœ… æ”¶é›†å®Œæˆ: {len(context['references'])} ä¸ªå¤–éƒ¨æ ‡å‡†")
        
        return context
    
    def cross_verify(self) -> Dict:
        """ç¬¬ä¸‰é˜¶æ®µï¼šäº¤å‰éªŒè¯"""
        print("\nğŸ”¬ é˜¶æ®µ3: äº¤å‰éªŒè¯...")
        
        if not self.internal_facts:
            raise ValueError("å¿…é¡»å…ˆè¿è¡Œå†…éƒ¨äº‹å®æ”¶é›†é˜¶æ®µ")
        
        analysis = {
            "actual_state_summary": self._summarize_internal_state(),
            "gaps": [],
            "aligned": [],
            "deviations": [],
            "extensions": []
        }
        
        # æ¨¡æ‹Ÿå·®å¼‚åˆ†æ
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæœ‰å¤æ‚çš„å¯¹æ¯”é€»è¾‘
        
        # ç¤ºä¾‹ï¼šæ£€æŸ¥å¥‘çº¦ç‰ˆæœ¬ä¸€è‡´æ€§
        versions = set()
        for contract_id, contract_info in self.internal_facts["contracts"].items():
            version = contract_info.get("version", "unknown")
            versions.add(version)
        
        if len(versions) > 1:
            analysis["deviations"].append(Difference(
                category=DifferenceCategory.ALTERNATIVE_IMPLEMENTATION,
                description=f"å‘ç°å¤šä¸ªç‰ˆæœ¬: {', '.join(versions)}",
                internal_evidence=f"å¥‘çº¦æ•°é‡: {len(self.internal_facts['contracts'])}",
                external_reference="SemVer 2.0.0 è¦æ±‚ç»Ÿä¸€ç‰ˆæœ¬ç­–ç•¥",
                action="review"
            ))
        else:
            analysis["aligned"].append(Difference(
                category=DifferenceCategory.ALIGNED,
                description="å¥‘çº¦ç‰ˆæœ¬ä¸€è‡´",
                internal_evidence=f"æ‰€æœ‰å¥‘çº¦ç‰ˆæœ¬: {list(versions)[0] if versions else 'N/A'}",
                external_reference="SemVer 2.0.0",
                action=None
            ))
        
        print(f"  âœ… åˆ†æå®Œæˆ: {len(analysis['aligned'])} å¯¹é½, {len(analysis['deviations'])} å·®å¼‚")
        
        return analysis
    
    def check_forbidden_phrases(self, text: str) -> List[str]:
        """æ£€æŸ¥ç¦æ­¢ä½¿ç”¨çš„çŸ­è¯­"""
        violations = []
        
        for pattern, replacement in self.forbidden_phrases:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                violations.append({
                    "phrase": match.group(),
                    "suggested_replacement": replacement,
                    "position": match.start()
                })
        
        return violations
    
    def calculate_evidence_coverage(self, report_text: str) -> float:
        """è®¡ç®—è¯æ®è¦†ç›–ç‡"""
        evidence_pattern = r"\[è¯æ®:\s*[^\]]+\]"
        matches = re.findall(evidence_pattern, report_text)
        
        # ç»Ÿè®¡é™ˆè¿°æ•°é‡ï¼ˆä»¥å¥å·åˆ†éš”ï¼‰
        statements = report_text.split('ã€‚')
        non_empty_statements = [s for s in statements if s.strip()]
        
        if len(non_empty_statements) == 0:
            return 0.0
        
        coverage = (len(matches) / len(non_empty_statements)) * 100
        return min(coverage, 100.0)
    
    def generate_report(self, analysis: Dict) -> Dict:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        print("\nğŸ“ ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
        
        report = {
            "metadata": {
                "report_id": hashlib.md5(
                    datetime.utcnow().isoformat().encode()
                ).hexdigest(),
                "generated_at": datetime.utcnow().isoformat(),
                "pipeline_version": self.config["metadata"]["version"],
                "internal_state_hash": self.internal_facts.get("root_hash", "N/A")
            },
            "actual_state": {
                "contracts": self.internal_facts["contracts"],
                "statistics": self.internal_facts["statistics"]
            },
            "comparison_analysis": analysis,
            "disclaimers": [
                "æœ¬æŠ¥å‘ŠåŸºäºå†…éƒ¨å®é™…çŠ¶æ€ç”Ÿæˆ",
                "å¤–éƒ¨æ ‡å‡†ä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆå®Œæˆåº¦å£°æ˜",
                "å·®å¼‚åˆ†æåŸºäºäº‹å®æ”¶é›†æ—¶çš„ç³»ç»ŸçŠ¶æ€",
                "æ‰€æœ‰ç»“è®ºéƒ½åŸºäºå¯éªŒè¯çš„è¯æ®"
            ],
            "verification": {
                "evidence_coverage": 0.0,
                "has_unverified_claims": False,
                "passed_all_checks": len(self.validation_errors) == 0,
                "validation_errors": self.validation_errors,
                "validation_warnings": self.validation_warnings
            }
        }
        
        # æ£€æŸ¥è´¨é‡é—¨ç¦
        quality_gates_passed = True
        
        # æ£€æŸ¥è¯æ®è¦†ç›–ç‡ï¼ˆå°†åœ¨æŠ¥å‘Šç”Ÿæˆåè®¡ç®—ï¼‰
        # æ£€æŸ¥ç¦æ­¢çŸ­è¯­
        report_text = json.dumps(report, ensure_ascii=False)
        forbidden_violations = self.check_forbidden_phrases(report_text)
        if forbidden_violations:
            report["verification"]["has_unverified_claims"] = True
            report["verification"]["forbidden_phrase_violations"] = forbidden_violations
            quality_gates_passed = False
        
        # æ›´æ–°éªŒè¯çŠ¶æ€
        report["verification"]["quality_gates_passed"] = quality_gates_passed
        
        print(f"  âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        print(f"  ğŸ“Š è´¨é‡é—¨ç¦: {'é€šè¿‡' if quality_gates_passed else 'å¤±è´¥'}")
        if forbidden_violations:
            print(f"  âš ï¸  æ£€æµ‹åˆ° {len(forbidden_violations)} ä¸ªç¦æ­¢çŸ­è¯­")
        
        return report
    
    def _summarize_internal_state(self) -> Dict:
        """æ€»ç»“å†…éƒ¨çŠ¶æ€"""
        return {
            "contracts_count": len(self.internal_facts.get("contracts", {})),
            "total_files": len(self.internal_facts.get("sources", [])),
            "total_size_bytes": sum(s["size"] for s in self.internal_facts.get("sources", [])),
            "root_hash": self.internal_facts.get("root_hash", "N/A")
        }
    
    def run_pipeline(self, topics: List[str] = None) -> Dict:
        """è¿è¡Œå®Œæ•´çš„ä¸‰é˜¶æ®µç®¡é“"""
        print("=" * 60)
        print("GL äº‹å®éªŒè¯ç®¡é“")
        print("=" * 60)
        
        # é˜¶æ®µ1: å†…éƒ¨äº‹å®æ”¶é›†
        facts = self.collect_internal_facts()
        
        if self.validation_errors:
            print(f"\nâŒ é˜¶æ®µ1 å¤±è´¥: å‘ç° {len(self.validation_errors)} ä¸ªé”™è¯¯")
            for error in self.validation_errors:
                print(f"  - {error}")
            # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œå¯èƒ½ä¼šå¤±è´¥å¹¶é€€å‡º
            # ä½†ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ç»§ç»­æ‰§è¡Œ
        
        # é˜¶æ®µ2: å¤–éƒ¨è¯­å¢ƒæ”¶é›†
        context = self.collect_external_context(topics)
        
        # é˜¶æ®µ3: äº¤å‰éªŒè¯
        analysis = self.cross_verify()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(analysis)
        
        print("\n" + "=" * 60)
        print("ç®¡é“æ‰§è¡Œå®Œæˆ")
        print("=" * 60)
        
        return report
    
    def save_report(self, report: Dict, output_path: str = None):
        """ä¿å­˜æŠ¥å‘Š"""
        if output_path is None:
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            output_path = f"gl-assessment-{timestamp}.json"
        
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        return output_file


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="GLäº‹å®éªŒè¯ç®¡é“æ‰§è¡Œè„šæœ¬"
    )
    parser.add_argument(
        "--config",
        default="ecosystem/contracts/fact-verification/gl.fact-pipeline-spec.yaml",
        help="ç®¡é“é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--workspace",
        default=".",
        help="å·¥ä½œç©ºé—´è·¯å¾„"
    )
    parser.add_argument(
        "--topics",
        nargs="+",
        default=["semver", "cncf", "togaf"],
        help="å¤–éƒ¨æ ‡å‡†ä¸»é¢˜"
    )
    parser.add_argument(
        "--output",
        help="æŠ¥å‘Šè¾“å‡ºè·¯å¾„"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºç®¡é“å®ä¾‹
    pipeline = GLFactPipeline(
        config_path=args.config,
        workspace_path=args.workspace
    )
    
    # è¿è¡Œç®¡é“
    report = pipeline.run_pipeline(topics=args.topics)
    
    # ä¿å­˜æŠ¥å‘Š
    pipeline.save_report(report, args.output)
    
    # é€€å‡ºçŠ¶æ€
    if report["verification"]["quality_gates_passed"]:
        print("\nâœ… éªŒè¯é€šè¿‡")
        return 0
    else:
        print("\nâŒ éªŒè¯å¤±è´¥")
        return 1


if __name__ == "__main__":
    exit(main())