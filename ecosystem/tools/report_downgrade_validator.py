#!/usr/bin/env python3
"""
å ±å‘Šé™éšé©—è­‰å·¥å…·
Report Downgrade Validator

åŠŸèƒ½ï¼šæª¢æ¸¬ä¸¦é©—è­‰å ±å‘Šé™éšï¼Œç§»é™¤ä¸ç¬¦åˆ Era-1 ç‹€æ…‹çš„é«˜ç´šèªç¾©
"""

import os
import re
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Set
from datetime import datetime
from dataclasses import dataclass, field


@dataclass
class SemanticIssue:
    """èªç¾©å•é¡Œ"""
    line: int
    column: int
    severity: str  # CRITICAL, HIGH, MEDIUM, LOW
    issue_type: str
    description: str
    context: str
    suggested_fix: str


@dataclass
class DowngradeReport:
    """é™éšå ±å‘Š"""
    file_path: str
    total_issues: int = 0
    critical_issues: int = 0
    high_issues: int = 0
    medium_issues: int = 0
    low_issues: int = 0
    issues: List[SemanticIssue] = field(default_factory=list)
    downgrade_needed: bool = False
    downgrade_percentage: float = 0.0


class ReportDowngradeValidator:
    """å ±å‘Šé™éšé©—è­‰å™¨"""
    
    def __init__(self, workspace: str = "/workspace"):
        self.workspace = Path(workspace)
        self.ecosystem_root = self.workspace / "ecosystem"
        self.current_era = "1"
        self.current_layer = "Operational (Evidence Generation)"
        self.current_semantic_closure = "NO"
        
        # Era-1 ç¦æ­¢çš„è¡“èªå’Œæ¨¡å¼
        self.forbidden_terminology = {
            "CRITICAL": [
                # è™›æ§‹éšæ®µ
                r"Phase\s+[1-9]",
                r"éšæ®µ\s+[1-9]",
                r"ç¬¬\s+[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*éšæ®µ",
                
                # æœªå°å­˜å‰çš„ç¦æ­¢è¡“èª
                r"å®Œæ•´æ€§[ä¿è¨¼ä¿è¯]",
                r"å°å­˜",
                r"å¯†å°",
                r"é–å®š",
                
                # è™›å‡å®Œæˆè²æ˜
                r"æœ€çµ‚",
                r"å®Œæˆ[ç‹€çŠ¶æ€æ…‹]",
                r"çµæŸ",
                r"çµ‚é»",
            ],
            "HIGH": [
                # é«˜ç´šæ¶æ§‹è²æ˜
                r"å®Œå…¨[ä¸€è‡´æ€§ç»Ÿä¸€]",
                r"çµ±ä¸€æ²»ç†",
                r"å®Œæ•´é–‰ç’°",
                
                # éåº¦è²æ˜
                r"100%[ä¿ä¿è¨¼è¨¼]",
                r"å®Œç¾",
                r"ç„¡ç¼º[é™·ç‚¹]",
            ],
            "MEDIUM": [
                # ä¸ç¢ºå®šçš„æœªä¾†è²æ˜
                r"å°‡[ä¼šæœƒ]",
                r"æœªä¾†",
                r"é æœŸ",
                
                # ä¸»è§€è©•åƒ¹
                r"å„ªç§€",
                r"å®Œç¾",
                r"æœ€ä½³",
            ]
        }
        
        # Era-1 å…è¨±çš„èªè¨€æ¨¡å¼
        self.allowed_patterns = {
            "Era": r"Era\s*:\s*[01]",
            "Layer": r"Layer\s*:\s*Operational\s*\(Evidence\s+Generation\)",
            "SemanticClosure": r"Semantic\s+Closure\s*:\s*NO",
            "EraState": r"Era.*Bootstrap|Era.*åˆå§‹åŒ–|Era.*å•Ÿå‹•",
        }
        
        # éœ€è¦çš„å…ƒæ•¸æ“šå­—æ®µ
        self.required_metadata = {
            "Era": self.current_era,
            "Layer": self.current_layer,
            "SemanticClosure": self.current_semantic_closure,
        }
    
    def read_file(self, file_path: str) -> List[str]:
        """è®€å–æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.readlines()
        except Exception as e:
            print(f"âŒ è®€å–æ–‡ä»¶å¤±æ•—: {e}")
            return []
    
    def check_forbidden_terminology(self, lines: List[str]) -> List[SemanticIssue]:
        """æª¢æŸ¥ç¦æ­¢è¡“èª"""
        issues = []
        
        for line_num, line in enumerate(lines, 1):
            for severity, patterns in self.forbidden_terminology.items():
                for pattern in patterns:
                    matches = re.finditer(pattern, line, re.IGNORECASE | re.UNICODE)
                    for match in matches:
                        issue = SemanticIssue(
                            line=line_num,
                            column=match.start() + 1,
                            severity=severity,
                            issue_type="forbidden_terminology",
                            description=f"æª¢æ¸¬åˆ°ç¦æ­¢è¡“èª: '{match.group()}'",
                            context=line.strip(),
                            suggested_fix=self._suggest_fix(match.group(), severity)
                        )
                        issues.append(issue)
        
        return issues
    
    def check_allowed_patterns(self, lines: List[str]) -> List[SemanticIssue]:
        """æª¢æŸ¥å…è¨±çš„æ¨¡å¼ï¼ˆç¢ºä¿å­˜åœ¨ï¼‰"""
        issues = []
        file_content = ''.join(lines)
        
        for field, pattern in self.required_metadata.items():
            if not re.search(pattern, file_content, re.IGNORECASE):
                issue = SemanticIssue(
                    line=1,
                    column=1,
                    severity="CRITICAL",
                    issue_type="missing_metadata",
                    description=f"ç¼ºå°‘å¿…è¦çš„å…ƒæ•¸æ“šå­—æ®µ: {field} (æ‡‰ç‚º: {self.required_metadata[field]})",
                    context="æ–‡ä»¶é ‚éƒ¨",
                    suggested_fix=f"æ·»åŠ å…ƒæ•¸æ“š: {field}: {self.required_metadata[field]}"
                )
                issues.append(issue)
        
        return issues
    
    def check_phase_declarations(self, lines: List[str]) -> List[SemanticIssue]:
        """æª¢æŸ¥è™›æ§‹çš„éšæ®µè²æ˜"""
        issues = []
        
        phase_patterns = [
            r"Phase\s+[1-9]",
            r"éšæ®µ\s*[1-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]",
            r"ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*éšæ®µ",
        ]
        
        for line_num, line in enumerate(lines, 1):
            for pattern in phase_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    # æª¢æŸ¥æ˜¯å¦åœ¨å…è¨±çš„ä¸Šä¸‹æ–‡ä¸­ï¼ˆå¦‚ Era æè¿°ï¼‰
                    allowed_contexts = [r"Era", r"Evidence"]
                    if not any(re.search(ctx, line, re.IGNORECASE) for ctx in allowed_contexts):
                        issue = SemanticIssue(
                            line=line_num,
                            column=1,
                            severity="CRITICAL",
                            issue_type="fictional_phase",
                            description="æª¢æ¸¬åˆ°è™›æ§‹çš„éšæ®µè²æ˜ï¼ˆEra-1 æœªå®šç¾©éšæ®µï¼‰",
                            context=line.strip(),
                            suggested_fix="ç§»é™¤éšæ®µè²æ˜ï¼Œä½¿ç”¨ Era + Layer æè¿°ç‹€æ…‹"
                        )
                        issues.append(issue)
        
        return issues
    
    def check_completeness_claims(self, lines: List[str]) -> List[SemanticIssue]:
        """æª¢æŸ¥å®Œæ•´æ€§è²æ˜ï¼ˆæœªå°å­˜å‰ç¦æ­¢ï¼‰"""
        issues = []
        
        completeness_patterns = [
            r"å®Œæ•´æ€§[ä¿è¨¼ä¿è¯]",
            r"å®Œå…¨ä¸€è‡´",
            r"å®Œæ•´é–‰ç’°",
            r"100%",
        ]
        
        for line_num, line in enumerate(lines, 1):
            for pattern in completeness_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    issue = SemanticIssue(
                        line=line_num,
                        column=1,
                        severity="HIGH",
                        issue_type="completeness_claim",
                        description="æª¢æ¸¬åˆ°å®Œæ•´æ€§è²æ˜ï¼ˆEra-1 æœªå°å­˜å‰ç¦æ­¢ï¼‰",
                        context=line.strip(),
                        suggested_fix="æ”¹ç”¨æº–ç¢ºçš„ç™¾åˆ†æ¯”æˆ–æè¿°ï¼Œé¿å…çµ•å°åŒ–èªè¨€"
                    )
                    issues.append(issue)
        
        return issues
    
    def check_era_state_consistency(self, lines: List[str]) -> List[SemanticIssue]:
        """æª¢æŸ¥ Era ç‹€æ…‹ä¸€è‡´æ€§"""
        issues = []
        file_content = ''.join(lines)
        
        # æª¢æŸ¥ Era è²æ˜
        era_matches = re.findall(r"Era\s*[:ï¼š]\s*(\d+)", file_content, re.IGNORECASE)
        if era_matches:
            era_values = set(era_matches)
            if len(era_values) > 1:
                issue = SemanticIssue(
                    line=1,
                    column=1,
                    severity="HIGH",
                    issue_type="era_inconsistency",
                    description=f"Era è²æ˜ä¸ä¸€è‡´: {era_values}",
                    context="æ•´å€‹æ–‡ä»¶",
                    suggested_fix="çµ±ä¸€ Era è²æ˜ç‚º Era-1"
                )
                issues.append(issue)
            elif not era_values == {"1"}:
                issue = SemanticIssue(
                    line=1,
                    column=1,
                    severity="HIGH",
                    issue_type="era_mismatch",
                    description=f"Era è²æ˜éŒ¯èª¤: {era_values} (æ‡‰ç‚º Era-1)",
                    context="æ•´å€‹æ–‡ä»¶",
                    suggested_fix="ä¿®æ­£ Era è²æ˜ç‚º Era-1"
                )
                issues.append(issue)
        
        # æª¢æŸ¥ Semantic Closure
        if re.search(r"Semantic\s+Closure\s*[:ï¼š]\s*YES", file_content, re.IGNORECASE):
            issue = SemanticIssue(
                line=1,
                column=1,
                severity="CRITICAL",
                issue_type="semantic_closure_error",
                description="Semantic Closure è²æ˜éŒ¯èª¤ (Era-1 æ‡‰ç‚º NO)",
                context="æ•´å€‹æ–‡ä»¶",
                suggested_fix="ä¿®æ­£ Semantic Closure ç‚º NO"
            )
            issues.append(issue)
        
        return issues
    
    def _suggest_fix(self, text: str, severity: str) -> str:
        """å»ºè­°ä¿®å¾©æ–¹æ¡ˆ"""
        fixes = {
            "Phase": "ä½¿ç”¨ Era + Layer æè¿°ç³»çµ±ç‹€æ…‹",
            "éšæ®µ": "ä½¿ç”¨ Era + Layer æè¿°ç³»çµ±ç‹€æ…‹",
            "å®Œæ•´æ€§": "ä½¿ç”¨æº–ç¢ºçš„ç™¾åˆ†æ¯”æˆ–éƒ¨åˆ†å®Œæˆæè¿°",
            "å°å­˜": "ç§»é™¤æˆ–æ”¹ç”¨ã€Œå€™é¸ç‹€æ…‹ã€",
            "æœ€çµ‚": "æ”¹ç”¨ã€Œç•¶å‰ã€æˆ–ã€Œæœ€æ–°ã€",
            "å®Œæˆ": "æ”¹ç”¨ã€Œé€²è¡Œä¸­ã€æˆ–ã€Œéƒ¨åˆ†å®Œæˆã€",
            "100%": "ä½¿ç”¨å¯¦éš›ç™¾åˆ†æ¯”æˆ–æè¿°",
        }
        
        for key, fix in fixes.items():
            if key in text:
                return fix
        
        return "ç§»é™¤æˆ–æ”¹ç”¨ç¬¦åˆ Era-1 çš„èªè¨€"
    
    def calculate_downgrade_percentage(self, issues: List[SemanticIssue]) -> float:
        """è¨ˆç®—éœ€è¦é™éšçš„ç™¾åˆ†æ¯”"""
        if not issues:
            return 0.0
        
        # æ ¹æ“šåš´é‡ç´šåˆ¥è¨ˆç®—æ¬Šé‡
        weights = {
            "CRITICAL": 1.0,
            "HIGH": 0.7,
            "MEDIUM": 0.4,
            "LOW": 0.1
        }
        
        total_weight = sum(weights.get(issue.severity, 0.1) for issue in issues)
        max_weight = 100  # å‡è¨­æœ€å¤š 100 å€‹å•é¡Œ
        
        return min((total_weight / max_weight) * 100, 100.0)
    
    def generate_downgrade_report(self, issues: List[SemanticIssue], file_path: str) -> DowngradeReport:
        """ç”Ÿæˆé™éšå ±å‘Š"""
        report = DowngradeReport(file_path=file_path)
        
        for issue in issues:
            report.issues.append(issue)
            report.total_issues += 1
            
            if issue.severity == "CRITICAL":
                report.critical_issues += 1
            elif issue.severity == "HIGH":
                report.high_issues += 1
            elif issue.severity == "MEDIUM":
                report.medium_issues += 1
            elif issue.severity == "LOW":
                report.low_issues += 1
        
        report.downgrade_needed = report.critical_issues > 0 or report.high_issues > 0
        report.downgrade_percentage = self.calculate_downgrade_percentage(issues)
        
        return report
    
    def print_report(self, report: DowngradeReport):
        """æ‰“å°å ±å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“‰ å ±å‘Šé™éšé©—è­‰å ±å‘Š")
        print("Report Downgrade Validation Report")
        print("=" * 80)
        print(f"\nğŸ“ æ–‡ä»¶: {report.file_path}")
        print(f"ğŸ¯ Era: {self.current_era} (Evidence-Native Bootstrap)")
        print(f"ğŸ—ï¸  Layer: {self.current_layer}")
        print(f"ğŸ”’ Semantic Closure: {self.current_semantic_closure}")
        
        print("\n" + "-" * 80)
        print("ğŸ“Š å•é¡Œçµ±è¨ˆ")
        print("-" * 80)
        print(f"ç¸½å•é¡Œæ•¸: {report.total_issues}")
        print(f"ğŸ”´ CRITICAL: {report.critical_issues}")
        print(f"ğŸŸ  HIGH: {report.high_issues}")
        print(f"ğŸŸ¡ MEDIUM: {report.medium_issues}")
        print(f"ğŸŸ¢ LOW: {report.low_issues}")
        
        print(f"\néœ€è¦é™éš: {'æ˜¯' if report.downgrade_needed else 'å¦'}")
        print(f"é™éšç¨‹åº¦: {report.downgrade_percentage:.1f}%")
        
        if report.issues:
            print("\n" + "-" * 80)
            print("ğŸš¨ è©³ç´°å•é¡Œåˆ—è¡¨")
            print("-" * 80)
            
            for i, issue in enumerate(report.issues, 1):
                severity_emoji = {
                    "CRITICAL": "ğŸ”´",
                    "HIGH": "ğŸŸ ",
                    "MEDIUM": "ğŸŸ¡",
                    "LOW": "ğŸŸ¢"
                }.get(issue.severity, "âšª")
                
                print(f"\n{severity_emoji} å•é¡Œ #{i}")
                print(f"   ä½ç½®: ç¬¬ {issue.line} è¡Œ")
                print(f"   åš´é‡ç´šåˆ¥: {issue.severity}")
                print(f"   é¡å‹: {issue.issue_type}")
                print(f"   æè¿°: {issue.description}")
                print(f"   ä¸Šä¸‹æ–‡: {issue.context}")
                print(f"   å»ºè­°ä¿®å¾©: {issue.suggested_fix}")
        
        print("\n" + "=" * 80)
        print("å ±å‘ŠçµæŸ")
        print("=" * 80)
    
    def validate_report(self, file_path: str) -> DowngradeReport:
        """é©—è­‰å ±å‘Š"""
        lines = self.read_file(file_path)
        
        if not lines:
            return DowngradeReport(file_path=file_path, downgrade_needed=False)
        
        # é‹è¡Œæ‰€æœ‰æª¢æŸ¥
        all_issues = []
        
        all_issues.extend(self.check_forbidden_terminology(lines))
        all_issues.extend(self.check_allowed_patterns(lines))
        all_issues.extend(self.check_phase_declarations(lines))
        all_issues.extend(self.check_completeness_claims(lines))
        all_issues.extend(self.check_era_state_consistency(lines))
        
        # ç”Ÿæˆå ±å‘Š
        report = self.generate_downgrade_report(all_issues, file_path)
        
        return report
    
    def generate_downgrade_plan(self, report: DowngradeReport) -> List[str]:
        """ç”Ÿæˆé™éšè¨ˆåŠƒ"""
        plan = []
        
        if not report.downgrade_needed:
            plan.append("âœ… å ±å‘Šç¬¦åˆ Era-1 è¦ç¯„ï¼Œç„¡éœ€é™éš")
            return plan
        
        plan.append("ğŸ”§ é™éšè¨ˆåŠƒ:")
        plan.append("")
        
        # æŒ‰å„ªå…ˆç´šæ’åº
        priorities = ["CRITICAL", "HIGH", "MEDIUM", "LOW"]
        
        for severity in priorities:
            issues = [i for i in report.issues if i.severity == severity]
            if issues:
                plan.append(f"\n{severity} å„ªå…ˆç´š:")
                for issue in issues:
                    plan.append(f"  â€¢ ç¬¬ {issue.line} è¡Œ: {issue.suggested_fix}")
        
        return plan


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(
        description="å ±å‘Šé™éšé©—è­‰å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # é©—è­‰å ±å‘Š
  python report_downgrade_validator.py reports/my-report.md
  
  # ç”Ÿæˆé™éšè¨ˆåŠƒ
  python report_downgrade_validator.py reports/my-report.md --plan
  
  # JSON æ ¼å¼è¼¸å‡º
  python report_downgrade_validator.py reports/my-report.md --json
        """
    )
    
    parser.add_argument(
        "report_file",
        type=str,
        help="è¦é©—è­‰çš„å ±å‘Šæ–‡ä»¶"
    )
    
    parser.add_argument(
        "--workspace",
        type=str,
        default="/workspace",
        help="å·¥ä½œç©ºé–“è·¯å¾‘ï¼ˆé»˜èª: /workspaceï¼‰"
    )
    
    parser.add_argument(
        "--plan",
        action="store_true",
        help="ç”Ÿæˆé™éšè¨ˆåŠƒ"
    )
    
    parser.add_argument(
        "--json",
        action="store_true",
        help="ä»¥ JSON æ ¼å¼è¼¸å‡º"
    )
    
    args = parser.parse_args()
    
    # å‰µå»ºé©—è­‰å™¨
    validator = ReportDowngradeValidator(workspace=args.workspace)
    
    # é©—è­‰å ±å‘Š
    report = validator.validate_report(args.report_file)
    
    # è¼¸å‡ºçµæœ
    if args.json:
        # JSON è¼¸å‡º
        output = {
            "file_path": report.file_path,
            "total_issues": report.total_issues,
            "critical_issues": report.critical_issues,
            "high_issues": report.high_issues,
            "medium_issues": report.medium_issues,
            "low_issues": report.low_issues,
            "downgrade_needed": report.downgrade_needed,
            "downgrade_percentage": report.downgrade_percentage,
            "issues": [
                {
                    "line": i.line,
                    "column": i.column,
                    "severity": i.severity,
                    "issue_type": i.issue_type,
                    "description": i.description,
                    "context": i.context,
                    "suggested_fix": i.suggested_fix
                }
                for i in report.issues
            ]
        }
        print(json.dumps(output, indent=2, ensure_ascii=False))
    else:
        # æ–‡æœ¬è¼¸å‡º
        validator.print_report(report)
        
        if args.plan:
            plan = validator.generate_downgrade_plan(report)
            print("\n" + "=" * 80)
            print("ğŸ“‹ é™éšè¨ˆåŠƒ")
            print("=" * 80)
            print("\n".join(plan))
    
    # é€€å‡ºç¢¼
    sys.exit(1 if report.downgrade_needed else 0)


if __name__ == "__main__":
    main()