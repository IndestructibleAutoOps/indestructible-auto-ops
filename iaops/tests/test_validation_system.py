#!/usr/bin/env python3
"""
ä¼ä¸šçº§ä¸¥æ ¼å·¥ç¨‹éªŒè¯ç³»ç»Ÿ - å®Œæ•´æµ‹è¯•å¥—ä»¶
Enterprise Strict Validation System - Complete Test Suite

Tests:
1. Severity ordering and comparison
2. ValidationIssue creation and blocking detection
3. ValidatorResult aggregation
4. RegressionDetector numeric/structural/trend detection
5. WhitelistManager rule matching, suppression, expiry, audit
6. FileCheckValidator file integrity checks
7. PerformanceValidator benchmark execution
8. StrictValidator full pipeline integration
9. ValidationEngine high-level API

æ³¨æ„ï¼šæœ¬æµ‹è¯•å¥—ä»¶åŒæ—¶æ”¯æŒç›´æ¥è¿è¡Œå’Œ pytest è¿è¡Œ
"""

import json
import os
import shutil
import sys
import tempfile
from pathlib import Path

# pytest compatibility
import pytest

# ç¡®ä¿å¯ä»¥å¯¼å…¥ validation æ¨¡å—
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from validation.file_validator import FileCheckValidator
from validation.performance_validator import PerformanceValidator
from validation.regression_detector import RegressionDetector
from validation.strict_validator import StrictValidator, ValidationEngine
from validation.validator import (
    Severity,
    ValidationConfig,
    ValidationIssue,
    ValidationResult,
    ValidatorResult,
)
from validation.whitelist_manager import WhitelistManager, WhitelistRule

# â”€â”€ æµ‹è¯•è¾…åŠ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class TestContext:
    """æµ‹è¯•ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨åˆ›å»ºå’Œæ¸…ç†ä¸´æ—¶ç›®å½•"""

    def __init__(self):
        self.temp_dir = None
        self.passed = 0
        self.failed = 0
        self.errors = []

    def setup(self):
        self.temp_dir = tempfile.mkdtemp(prefix="validation_test_")
        # åˆ›å»ºåŸºæœ¬é¡¹ç›®ç»“æ„
        src_dir = Path(self.temp_dir) / "src"
        src_dir.mkdir(parents=True)
        (src_dir / "__init__.py").write_text("")
        (src_dir / "main.py").write_text("# main module\ndef main():\n    pass\n")
        (src_dir / "utils.py").write_text("# utils module\ndef helper():\n    pass\n")
        # åˆ›å»º README å’Œ pyproject.toml
        (Path(self.temp_dir) / "README.md").write_text("# Test Project\n")
        (Path(self.temp_dir) / "pyproject.toml").write_text('[project]\nname = "test"\n')
        return self

    def teardown(self):
        if self.temp_dir and os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

    def get_config(self, **overrides) -> ValidationConfig:
        defaults = {
            "project_root": self.temp_dir,
            "baseline_dir": os.path.join(self.temp_dir, ".baselines"),
            "output_dir": os.path.join(self.temp_dir, ".validation"),
            "strict_mode": True,
            "performance_threshold": 0.2,
            "metric_threshold": 0.1,
        }
        defaults.update(overrides)
        return ValidationConfig(**defaults)

    def assert_true(self, condition, message):
        if condition:
            self.passed += 1
            print(f"  âœ… {message}")
        else:
            self.failed += 1
            self.errors.append(message)
            print(f"  âŒ {message}")

    def assert_equal(self, actual, expected, message):
        self.assert_true(actual == expected, f"{message} (expected={expected}, actual={actual})")

    def assert_not_none(self, value, message):
        self.assert_true(value is not None, message)

    def assert_none(self, value, message):
        self.assert_true(value is None, message)

    def print_summary(self):
        total = self.passed + self.failed
        print(f"\n{'=' * 60}")
        print(f"  æµ‹è¯•ç»“æœ: {self.passed}/{total} é€šè¿‡")
        if self.failed > 0:
            print(f"  âŒ å¤±è´¥: {self.failed}")
            for err in self.errors:
                print(f"    - {err}")
        else:
            print("  âœ… å…¨éƒ¨é€šè¿‡!")
        print(f"{'=' * 60}")
        return self.failed == 0


# â”€â”€ Pytest å…¼å®¹æ€§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@pytest.fixture
def ctx():
    """Pytest fixture for test context"""
    test_ctx = TestContext()
    test_ctx.setup()
    yield test_ctx
    test_ctx.teardown()


# â”€â”€ æµ‹è¯•ç”¨ä¾‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def test_severity(ctx: TestContext):
    """æµ‹è¯•ä¸¥é‡çº§åˆ«å®šä¹‰å’Œæ’åº"""
    print("\nğŸ“‹ æµ‹è¯• 1: Severity ä¸¥é‡çº§åˆ«")

    order = Severity.severity_order()
    ctx.assert_equal(len(order), 5, "ä¸¥é‡çº§åˆ«å…±5ä¸ª")
    ctx.assert_equal(order[0], Severity.BLOCKER, "æœ€é«˜çº§åˆ«ä¸º BLOCKER")
    ctx.assert_equal(order[-1], Severity.INFO, "æœ€ä½çº§åˆ«ä¸º INFO")

    ctx.assert_true(Severity.is_blocking(Severity.BLOCKER), "BLOCKER æ˜¯é˜»å¡æ€§çš„")
    ctx.assert_true(Severity.is_blocking(Severity.CRITICAL), "CRITICAL æ˜¯é˜»å¡æ€§çš„")
    ctx.assert_true(Severity.is_blocking(Severity.ERROR), "ERROR æ˜¯é˜»å¡æ€§çš„")
    ctx.assert_true(not Severity.is_blocking(Severity.WARNING), "WARNING ä¸æ˜¯é˜»å¡æ€§çš„")
    ctx.assert_true(not Severity.is_blocking(Severity.INFO), "INFO ä¸æ˜¯é˜»å¡æ€§çš„")


def test_validation_issue(ctx: TestContext):
    """æµ‹è¯• ValidationIssue æ•°æ®ç»“æ„"""
    print("\nğŸ“‹ æµ‹è¯• 2: ValidationIssue æ•°æ®ç»“æ„")

    issue = ValidationIssue(
        issue_id="test_001",
        description="æµ‹è¯•é—®é¢˜",
        severity=Severity.CRITICAL,
        details={"key": "value"},
        category="test",
        source="test_module",
    )

    ctx.assert_equal(issue.issue_id, "test_001", "issue_id æ­£ç¡®")
    ctx.assert_true(issue.is_blocking(), "CRITICAL é—®é¢˜æ˜¯é˜»å¡æ€§çš„")

    d = issue.to_dict()
    ctx.assert_equal(d["severity"], "CRITICAL", "to_dict åºåˆ—åŒ–æ­£ç¡®")

    info_issue = ValidationIssue(
        issue_id="test_002",
        description="ä¿¡æ¯",
        severity=Severity.INFO,
        details={},
    )
    ctx.assert_true(not info_issue.is_blocking(), "INFO é—®é¢˜ä¸æ˜¯é˜»å¡æ€§çš„")


def test_validator_result(ctx: TestContext):
    """æµ‹è¯• ValidatorResult èšåˆ"""
    print("\nğŸ“‹ æµ‹è¯• 3: ValidatorResult èšåˆ")

    issues = [
        ValidationIssue("i1", "blocker", Severity.BLOCKER, {}),
        ValidationIssue("i2", "critical", Severity.CRITICAL, {}),
        ValidationIssue("i3", "warning", Severity.WARNING, {}),
    ]

    result = ValidatorResult(
        validator_name="test_validator",
        passed=False,
        issues=issues,
    )

    ctx.assert_equal(result.issue_count, 3, "é—®é¢˜æ€»æ•°ä¸º 3")
    ctx.assert_equal(result.blocking_count, 2, "é˜»å¡æ€§é—®é¢˜ä¸º 2")
    ctx.assert_equal(len(result.blocking_issues), 2, "blocking_issues åˆ—è¡¨é•¿åº¦ä¸º 2")

    d = result.to_dict()
    ctx.assert_equal(d["total_count"], 3, "to_dict total_count æ­£ç¡®")


def test_regression_numeric(ctx: TestContext):
    """æµ‹è¯•æ•°å€¼å‹å›å½’æ£€æµ‹"""
    print("\nğŸ“‹ æµ‹è¯• 4: RegressionDetector æ•°å€¼æ£€æµ‹")

    config = ctx.get_config()
    detector = RegressionDetector(config)

    # æ€§èƒ½é€€åŒ–: å½“å‰å€¼è¶…è¿‡åŸºçº¿ 20%
    issue = detector.detect_numeric(1.3, 1.0, "performance", "api_latency")
    ctx.assert_not_none(issue, "æ£€æµ‹åˆ°æ€§èƒ½é€€åŒ– (30% > 20%)")
    ctx.assert_equal(issue.severity, Severity.CRITICAL, "æ€§èƒ½é€€åŒ–ä¸º CRITICAL")

    # æ€§èƒ½æ­£å¸¸: å½“å‰å€¼åœ¨é˜ˆå€¼å†…
    issue = detector.detect_numeric(1.1, 1.0, "performance", "api_latency")
    ctx.assert_none(issue, "æ€§èƒ½æ­£å¸¸ (10% < 20%)")

    # é€šç”¨æŒ‡æ ‡é€€åŒ–: å½“å‰å€¼ä½äºåŸºçº¿ 10%
    issue = detector.detect_numeric(0.85, 1.0, "general", "coverage")
    ctx.assert_not_none(issue, "æ£€æµ‹åˆ°æŒ‡æ ‡ä¸‹é™ (15% > 10%)")

    # é€šç”¨æŒ‡æ ‡æ­£å¸¸
    issue = detector.detect_numeric(0.95, 1.0, "general", "coverage")
    ctx.assert_none(issue, "æŒ‡æ ‡æ­£å¸¸ (5% < 10%)")

    # None å€¼å¤„ç†
    issue = detector.detect_numeric(None, 1.0, "general", "test")
    ctx.assert_none(issue, "None å½“å‰å€¼è¿”å› None")

    issue = detector.detect_numeric(1.0, None, "general", "test")
    ctx.assert_none(issue, "None åŸºçº¿å€¼è¿”å› None")


def test_regression_structural(ctx: TestContext):
    """æµ‹è¯•ç»“æ„å˜åŒ–æ£€æµ‹"""
    print("\nğŸ“‹ æµ‹è¯• 5: RegressionDetector ç»“æ„æ£€æµ‹")

    config = ctx.get_config()
    detector = RegressionDetector(config)

    # é”®å·®å¼‚
    issue = detector.detect_structural({"a": 1, "c": 3}, {"a": 1, "b": 2}, "test_api")
    ctx.assert_not_none(issue, "æ£€æµ‹åˆ°é”®å·®å¼‚")
    ctx.assert_equal(issue.severity, Severity.BLOCKER, "ç»“æ„å˜åŒ–ä¸º BLOCKER")

    # ç±»å‹å˜åŒ–
    issue = detector.detect_structural({"a": "string"}, {"a": 123}, "test_api")
    ctx.assert_not_none(issue, "æ£€æµ‹åˆ°ç±»å‹å˜åŒ–")

    # ç»“æ„ä¸€è‡´
    issue = detector.detect_structural({"a": 1, "b": 2}, {"a": 1, "b": 2}, "test_api")
    ctx.assert_none(issue, "ç»“æ„ä¸€è‡´æ— é—®é¢˜")

    # ç©ºåŸºçº¿
    issue = detector.detect_structural({"a": 1}, {}, "test_api")
    ctx.assert_none(issue, "ç©ºåŸºçº¿è¿”å› None")


def test_regression_trend(ctx: TestContext):
    """æµ‹è¯•è¶‹åŠ¿é€€åŒ–æ£€æµ‹"""
    print("\nğŸ“‹ æµ‹è¯• 6: RegressionDetector è¶‹åŠ¿æ£€æµ‹")

    config = ctx.get_config()
    detector = RegressionDetector(config)

    # æŒç»­ä¸‹é™è¶‹åŠ¿
    declining = [100, 90, 80, 70, 60]
    issue = detector.detect_trend(declining, "test_metric", window=5)
    ctx.assert_not_none(issue, "æ£€æµ‹åˆ°æŒç»­ä¸‹é™è¶‹åŠ¿")

    # ç¨³å®šè¶‹åŠ¿
    stable = [100, 101, 99, 100, 101]
    issue = detector.detect_trend(stable, "test_metric", window=5)
    ctx.assert_none(issue, "ç¨³å®šè¶‹åŠ¿æ— é—®é¢˜")

    # æ•°æ®ä¸è¶³
    short = [100, 90]
    issue = detector.detect_trend(short, "test_metric", window=5)
    ctx.assert_none(issue, "æ•°æ®ä¸è¶³è¿”å› None")


def test_regression_batch(ctx: TestContext):
    """æµ‹è¯•æ‰¹é‡å›å½’æ£€æµ‹"""
    print("\nğŸ“‹ æµ‹è¯• 7: RegressionDetector æ‰¹é‡æ£€æµ‹")

    config = ctx.get_config()
    detector = RegressionDetector(config)

    current = {"latency": 1.5, "coverage": 0.7, "throughput": 900}
    baseline = {"latency": 1.0, "coverage": 0.9, "throughput": 1000}
    types = {"latency": "performance", "coverage": "general", "throughput": "general"}

    issues = detector.detect_all(current, baseline, types)
    ctx.assert_true(len(issues) >= 2, f"æ‰¹é‡æ£€æµ‹å‘ç° {len(issues)} ä¸ªé—®é¢˜ (>=2)")


def test_whitelist_basic(ctx: TestContext):
    """æµ‹è¯•ç™½åå•åŸºæœ¬åŠŸèƒ½"""
    print("\nğŸ“‹ æµ‹è¯• 8: WhitelistManager åŸºæœ¬åŠŸèƒ½")

    manager = WhitelistManager()

    rule = WhitelistRule(
        rule_id="WL-TEST-001",
        description="æµ‹è¯•è§„åˆ™",
        pattern="performance_.*",
        max_severity="CRITICAL",
        approved_by="tester",
    )
    manager.add_rule(rule)

    ctx.assert_equal(len(manager.rules), 1, "æ·»åŠ è§„åˆ™æˆåŠŸ")
    ctx.assert_equal(len(manager.get_active_rules()), 1, "æ´»è·ƒè§„åˆ™ä¸º 1")

    # åŒ¹é…æµ‹è¯•
    issue = ValidationIssue(
        issue_id="performance_regression_api",
        description="æ€§èƒ½é€€åŒ–",
        severity=Severity.CRITICAL,
        details={},
        category="performance",
    )
    ctx.assert_true(rule.matches(issue), "è§„åˆ™åŒ¹é… CRITICAL æ€§èƒ½é—®é¢˜")

    # BLOCKER ä¸å¯è±å…
    blocker = ValidationIssue(
        issue_id="performance_regression_api",
        description="é˜»å¡æ€§é—®é¢˜",
        severity=Severity.BLOCKER,
        details={},
    )
    ctx.assert_true(not rule.matches(blocker), "BLOCKER ä¸å¯è±å…")


def test_whitelist_suppression(ctx: TestContext):
    """æµ‹è¯•ç™½åå•æŠ‘åˆ¶åŠŸèƒ½"""
    print("\nğŸ“‹ æµ‹è¯• 9: WhitelistManager æŠ‘åˆ¶åŠŸèƒ½")

    manager = WhitelistManager()
    manager.add_rule(
        WhitelistRule(
            rule_id="WL-SUPP-001",
            description="æŠ‘åˆ¶æ€§èƒ½é—®é¢˜",
            pattern="performance_.*",
            max_severity="CRITICAL",
            approved_by="tester",
        )
    )

    issues = [
        ValidationIssue("performance_regression_api", "æ€§èƒ½é€€åŒ–", Severity.CRITICAL, {}),
        ValidationIssue("structural_change_api", "ç»“æ„å˜åŒ–", Severity.BLOCKER, {}),
        ValidationIssue("metric_regression_cov", "è¦†ç›–ç‡ä¸‹é™", Severity.WARNING, {}),
    ]

    processed = manager.apply_rules(issues)
    ctx.assert_equal(len(processed), 3, "å¤„ç†åé—®é¢˜æ•°ä¸å˜")

    # æ€§èƒ½é—®é¢˜è¢«æŠ‘åˆ¶ä¸º INFO
    ctx.assert_equal(processed[0].severity, Severity.INFO, "æ€§èƒ½é—®é¢˜è¢«æŠ‘åˆ¶ä¸º INFO")
    ctx.assert_true("[SUPPRESSED]" in processed[0].description, "æè¿°åŒ…å« [SUPPRESSED]")

    # BLOCKER ä¸å—å½±å“
    ctx.assert_equal(processed[1].severity, Severity.BLOCKER, "BLOCKER ä¸å—å½±å“")

    # ä¸åŒ¹é…çš„é—®é¢˜ä¸å—å½±å“
    ctx.assert_equal(processed[2].severity, Severity.WARNING, "ä¸åŒ¹é…çš„é—®é¢˜ä¸å—å½±å“")

    ctx.assert_equal(manager.suppression_count, 1, "æŠ‘åˆ¶è®¡æ•°ä¸º 1")


def test_whitelist_expiry(ctx: TestContext):
    """æµ‹è¯•ç™½åå•è¿‡æœŸåŠŸèƒ½"""
    print("\nğŸ“‹ æµ‹è¯• 10: WhitelistManager è¿‡æœŸè§„åˆ™")

    manager = WhitelistManager()

    # å·²è¿‡æœŸè§„åˆ™
    expired_rule = WhitelistRule(
        rule_id="WL-EXP-001",
        description="å·²è¿‡æœŸè§„åˆ™",
        pattern="performance_.*",
        max_severity="CRITICAL",
        expires_at="2020-01-01T00:00:00",
        approved_by="tester",
    )
    manager.add_rule(expired_rule)

    ctx.assert_true(expired_rule.is_expired(), "è§„åˆ™å·²è¿‡æœŸ")
    ctx.assert_equal(len(manager.get_expired_rules()), 1, "è¿‡æœŸè§„åˆ™åˆ—è¡¨ä¸º 1")
    ctx.assert_equal(len(manager.get_active_rules()), 0, "æ´»è·ƒè§„åˆ™åˆ—è¡¨ä¸º 0")

    # è¿‡æœŸè§„åˆ™ä¸åº”æŠ‘åˆ¶é—®é¢˜
    issues = [
        ValidationIssue("performance_regression_api", "æ€§èƒ½é€€åŒ–", Severity.CRITICAL, {}),
    ]
    processed = manager.apply_rules(issues)
    ctx.assert_equal(processed[0].severity, Severity.CRITICAL, "è¿‡æœŸè§„åˆ™ä¸æŠ‘åˆ¶é—®é¢˜")


def test_file_validator(ctx: TestContext):
    """æµ‹è¯•æ–‡ä»¶æ£€æŸ¥å™¨"""
    print("\nğŸ“‹ æµ‹è¯• 11: FileCheckValidator æ–‡ä»¶æ£€æŸ¥")

    config = ctx.get_config()

    # é¦–æ¬¡è¿è¡Œï¼ˆæ— åŸºçº¿ï¼‰
    validator = FileCheckValidator(config)
    result = validator.execute()
    ctx.assert_true(result.passed, "é¦–æ¬¡è¿è¡Œé€šè¿‡ï¼ˆæ— åŸºçº¿ï¼‰")

    # æ¨¡æ‹Ÿæ–‡ä»¶å‡å°‘
    baseline = {"source_file_count": 10}
    validator2 = FileCheckValidator(config, baseline=baseline)
    result2 = validator2.execute()
    # å½“å‰åªæœ‰ 3 ä¸ª py æ–‡ä»¶ï¼ŒåŸºçº¿æ˜¯ 10ï¼Œåº”è¯¥æ£€æµ‹åˆ°å‡å°‘
    has_count_issue = any("source_file_count" in i.issue_id for i in result2.issues)
    ctx.assert_true(has_count_issue, "æ£€æµ‹åˆ°æºæ–‡ä»¶æ•°é‡å‡å°‘")


def test_file_validator_required(ctx: TestContext):
    """æµ‹è¯•å¿…éœ€æ–‡ä»¶æ£€æŸ¥"""
    print("\nğŸ“‹ æµ‹è¯• 12: FileCheckValidator å¿…éœ€æ–‡ä»¶")

    # åˆ›å»ºç¼ºå°‘ README çš„é¡¹ç›®
    temp_dir = tempfile.mkdtemp(prefix="validation_test_noreq_")
    try:
        Path(temp_dir).joinpath("src").mkdir()
        Path(temp_dir).joinpath("src", "main.py").write_text("pass")
        # ä¸åˆ›å»º README.md

        config = ValidationConfig(
            project_root=temp_dir,
            baseline_dir=os.path.join(temp_dir, ".baselines"),
            output_dir=os.path.join(temp_dir, ".validation"),
        )
        validator = FileCheckValidator(config)
        result = validator.execute()

        has_missing = any("missing_required_file" in i.issue_id for i in result.issues)
        ctx.assert_true(has_missing, "æ£€æµ‹åˆ°ç¼ºå¤±å¿…éœ€æ–‡ä»¶")
    finally:
        shutil.rmtree(temp_dir)


def test_performance_validator(ctx: TestContext):
    """æµ‹è¯•æ€§èƒ½éªŒè¯å™¨"""
    print("\nğŸ“‹ æµ‹è¯• 13: PerformanceValidator æ€§èƒ½éªŒè¯")

    config = ctx.get_config()
    validator = PerformanceValidator(config)

    # æ³¨å†Œä¸€ä¸ªç®€å•çš„åŸºå‡†æµ‹è¯•
    validator.register_benchmark("simple_sort", lambda: sorted(range(1000, 0, -1)))
    result = validator.execute()
    ctx.assert_true(isinstance(result, ValidatorResult), "è¿”å› ValidatorResult")
    ctx.assert_equal(result.validator_name, "performance_validator", "éªŒè¯å™¨åç§°æ­£ç¡®")


def test_performance_with_regression(ctx: TestContext):
    """æµ‹è¯•æ€§èƒ½é€€åŒ–æ£€æµ‹"""
    print("\nğŸ“‹ æµ‹è¯• 14: PerformanceValidator é€€åŒ–æ£€æµ‹")

    config = ctx.get_config()

    # æ‰‹åŠ¨è®¾ç½®åŸºçº¿ä½¿å…¶è§¦å‘é€€åŒ–
    baseline = {
        "performance_metrics": {
            "api_latency": {
                "current": 0.5,
                "baseline": 0.2,
                "type": "performance",
            }
        }
    }
    validator = PerformanceValidator(config, baseline=baseline)
    result = validator.execute()

    has_regression = any("regression" in i.issue_id for i in result.issues)
    ctx.assert_true(has_regression, "æ£€æµ‹åˆ°æ€§èƒ½é€€åŒ–")


def test_strict_validator_pipeline(ctx: TestContext):
    """æµ‹è¯•å®Œæ•´éªŒè¯ç®¡é“"""
    print("\nğŸ“‹ æµ‹è¯• 15: StrictValidator å®Œæ•´ç®¡é“")

    config = ctx.get_config()
    validator = StrictValidator(config)
    result = validator.validate()

    ctx.assert_true(isinstance(result, ValidationResult), "è¿”å› ValidationResult")
    ctx.assert_true(result.timestamp > 0, "æ—¶é—´æˆ³æœ‰æ•ˆ")
    ctx.assert_true("file_validator" in result.validators, "åŒ…å« file_validator")

    # æ£€æŸ¥æŠ¥å‘Šä¿å­˜
    report_dir = Path(config.output_dir)
    reports = list(report_dir.glob("report_*.json"))
    ctx.assert_true(len(reports) > 0, "æŠ¥å‘Šæ–‡ä»¶å·²ä¿å­˜")


def test_validation_engine(ctx: TestContext):
    """æµ‹è¯•é«˜å±‚éªŒè¯å¼•æ“"""
    print("\nğŸ“‹ æµ‹è¯• 16: ValidationEngine é«˜å±‚ API")

    config = ctx.get_config()
    engine = ValidationEngine(config)
    result = engine.run()

    ctx.assert_true(isinstance(result, ValidationResult), "è¿”å› ValidationResult")
    summary = result.get_summary()
    ctx.assert_true("total" in summary, "æ‘˜è¦åŒ…å« total")
    ctx.assert_true("blocking" in summary, "æ‘˜è¦åŒ…å« blocking")


def test_validation_result_serialization(ctx: TestContext):
    """æµ‹è¯•éªŒè¯ç»“æœåºåˆ—åŒ–"""
    print("\nğŸ“‹ æµ‹è¯• 17: ValidationResult åºåˆ—åŒ–")

    config = ctx.get_config()
    engine = ValidationEngine(config)
    result = engine.run()

    # æµ‹è¯• to_dict
    d = result.to_dict()
    ctx.assert_true(isinstance(d, dict), "to_dict è¿”å›å­—å…¸")
    ctx.assert_true("validators" in d, "åŒ…å« validators")
    ctx.assert_true("summary" in d, "åŒ…å« summary")

    # æµ‹è¯• JSON åºåˆ—åŒ–
    json_str = json.dumps(d, ensure_ascii=False)
    ctx.assert_true(len(json_str) > 0, "JSON åºåˆ—åŒ–æˆåŠŸ")

    # ååºåˆ—åŒ–éªŒè¯
    parsed = json.loads(json_str)
    ctx.assert_equal(parsed["overall_passed"], d["overall_passed"], "ååºåˆ—åŒ–ä¸€è‡´")


def test_whitelist_persistence(ctx: TestContext):
    """æµ‹è¯•ç™½åå•æŒä¹…åŒ–"""
    print("\nğŸ“‹ æµ‹è¯• 18: WhitelistManager æŒä¹…åŒ–")

    wl_path = os.path.join(ctx.temp_dir, "test_whitelist.json")

    # åˆ›å»ºå¹¶ä¿å­˜
    manager = WhitelistManager(wl_path)
    manager.add_rule(
        WhitelistRule(
            rule_id="WL-PERSIST-001",
            description="æŒä¹…åŒ–æµ‹è¯•",
            pattern="test_.*",
            max_severity="ERROR",
            approved_by="tester",
        )
    )
    manager.save_rules()

    # é‡æ–°åŠ è½½
    manager2 = WhitelistManager(wl_path)
    ctx.assert_equal(len(manager2.rules), 1, "é‡æ–°åŠ è½½è§„åˆ™æ•°é‡æ­£ç¡®")
    ctx.assert_equal(
        list(manager2.rules.values())[0].rule_id,
        "WL-PERSIST-001",
        "è§„åˆ™ ID æ­£ç¡®",
    )


# â”€â”€ ä¸»æ‰§è¡Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def main():
    print("=" * 60)
    print("  ä¼ä¸šçº§ä¸¥æ ¼å·¥ç¨‹éªŒè¯ç³»ç»Ÿ - æµ‹è¯•å¥—ä»¶")
    print("  Enterprise Strict Validation - Test Suite")
    print("=" * 60)

    ctx = TestContext()
    ctx.setup()

    try:
        test_severity(ctx)
        test_validation_issue(ctx)
        test_validator_result(ctx)
        test_regression_numeric(ctx)
        test_regression_structural(ctx)
        test_regression_trend(ctx)
        test_regression_batch(ctx)
        test_whitelist_basic(ctx)
        test_whitelist_suppression(ctx)
        test_whitelist_expiry(ctx)
        test_file_validator(ctx)
        test_file_validator_required(ctx)
        test_performance_validator(ctx)
        test_performance_with_regression(ctx)
        test_strict_validator_pipeline(ctx)
        test_validation_engine(ctx)
        test_validation_result_serialization(ctx)
        test_whitelist_persistence(ctx)
    finally:
        ctx.teardown()

    all_passed = ctx.print_summary()
    sys.exit(0 if all_passed else 1)


if __name__ == "__main__":
    main()
